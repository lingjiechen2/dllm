<h1 align="center">dLLM</h1>

<p align="center">
Simple Diffusion Language Modeling
</p>

<p align="center">
<img
  src="assets/logo.gif"
  alt="dLLM logo">
</p>


## Overview
**dLLM** is an educational library offering unified implementations for training **diffusion language models**. It brings transparency to the entire training and deployment process, making **reproduction and finetuning** of open-weight diffusion language models much easier. Below are some of the key features that make dLLM special:

- dLLM provides reproduction and finetuning recipes for a variety of open-weight models (e.g., [LLaDA](https://arxiv.org/abs/2502.09992), [Dream](https://arxiv.org/abs/2508.15487) and [RND1](https://www.radicalnumerics.ai/assets/rnd1_report.pdf)), and provides reference implementation of various training algorithms (e.g., [Edit Flows](https://arxiv.org/abs/2506.09018)).

- dLLM, built on top of [ðŸ¤— Transformers](https://github.com/huggingface/transformers), scales seamlesslyâ€”from edge devices with [LoRA](https://github.com/huggingface/peft) to multi-node clusters with [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and beyond.

- dLLM provides unified, modular training pipelines (inspired by [ðŸ¤— Transformers Trainer](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py)) and well-documented [examples](/examples/), making customization simple and development highly user-friendly.

> [!NOTE]
> This repository is primarily for educational purposes and does not aim for 100% exact reproduction of official models (which is impossible). We hope it serves as a helpful reference for the community â€” contributions and improvements are always welcome!


## Table of Contents
<!-- - [Overview](#overview) -->
- [Features & Documentation](#features--documentations)
- [Setup](#setup)
- [Files overview](#files-overview)
- [Training](#training)
- [Inference](#inference)
- [Evaluation](#evaluation)
- [Citation](#citation)


## Features & Documentations

1. [`examples/llada`](/examples/llada): Finetuning open-weight LLaDA [LLaDA](https://arxiv.org/abs/2502.09992) / [LLaDA-MoE](https://arxiv.org/abs/2509.24389), as well as reproducing LLaDA by training from scratch on public data (pretraining & finetuning).
2. [`examples/dream`](/examples/dream): Finetuning open-weight Dream [Dream](https://arxiv.org/abs/2508.15487), as well as reproducing Dream by training from scratch on public data (pretraining & finetuning).
3. [`examples/rnd`](/examples/rnd): (WIP) Finetuning open-weight RND1 [RND1-Base](https://www.radicalnumerics.ai/assets/rnd1_report.pdf).
4. [`examples/editflow`](/examples/editflow): Educational reference for training [EditFlow](https://arxiv.org/abs/2506.09018) models, demonstrating how to extend existing DLLMs (e.g., LLaDA and Dream) with *edit operations*â€”insertion, deletion, and substitutionâ€”and how to pretrain or finetune EditFlow models from scratch on public data.

   <details>
   <summary>ðŸŽ¬ Click to show EditFlow Demo</summary>

   <p align="center">
     <img src="/examples/editflow/assets/all.gif" alt="EditFlow demo" width="100%">
   </p>
   <p align="center"><em>EditFlow performing insertion (blue), substitution from mask tokens (black), substitution from non-mask tokens (red), and deletion (strikethrough â†’ removed) during generation.</em></p>

   </details>
- More upcoming.


## Setup
### Installation
```bash
# create and activate conda environment
conda create -n dllm python=3.10 -y
conda activate dllm

# install pytorch with CUDA 12.4 (other pytorch/cuda versions should also work)
conda install cuda=12.4 -c nvidia
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 \
    --index-url https://download.pytorch.org/whl/cu124

# install dllm package
pip install -e .
```
### (optional) Evaluation setup

```bash
# initialize `lm-evaluation-harness` submodule
git submodule update --init --recursive

# install submodule in editable mode with IFEval & Math dependencies
pip install -e "lm-evaluation-harness[ifeval,math]"
```

### (optional) Slurm setup
For [Slurm](https://slurm.schedmd.com/) users, update [`scripts/train.slurm.sh`](/scripts/train.slurm.sh) for your cluster:
```diff
- #SBATCH --partition=mllm_safety # Note: adjust this for your cluster
- #SBATCH --quotatype=spot        # Note: adjust this for your cluster
+ #SBATCH --partition=YOUR_PARTITION
+ #SBATCH --quotatype=YOUR_QUOTATYPE
```
Next, create a directory for your job logs:
```shell
mkdir logs
```
This folder will store the log files generated by your sbatch jobs.

## Files overview
```
# modules for training / sampling
dllm
â”œâ”€â”€ core                   # Core reusable modules shared across `dllm/pipelines` 
â”‚   â”œâ”€â”€ generation
â”‚   â”œâ”€â”€ schedulers
â”‚   â””â”€â”€ trainers
â”œâ”€â”€ data
â”œâ”€â”€ pipelines              # Application-specific training & inference pipelines
|   â”œâ”€â”€ bert
â”‚   â”œâ”€â”€ dream
â”‚   â”œâ”€â”€ editflow
â”‚   â””â”€â”€ llada
â”‚       â”œâ”€â”€ models         # Model architecture and configs 
â”‚       â”œâ”€â”€ generator.py   # Generation utilities
â”‚       â”œâ”€â”€ trainer.py     # Core training logic
â”‚       â””â”€â”€ eval.py        # Evaluation entry point
â”œâ”€â”€ tools
â””â”€â”€ utils

# entry points for training / sampling
examples
â”œâ”€â”€ bert
â”œâ”€â”€ dream
â”œâ”€â”€ editflow
â””â”€â”€ llada
    â”œâ”€â”€ chat.py            # Interactive inference example
    â”œâ”€â”€ generate.py        # Inference example
    â”œâ”€â”€ pt.py              # Pretraining example
    â”œâ”€â”€ README.md          # Documentation (you are here)
    â”œâ”€â”€ sft.py             # Supervised finetuning example
    â””â”€â”€ eval.sh            # Evalution script
```

## Training

A typical training entry script looks like (for example, [`examples/llada/sft.py`](/examples/llada/sft.py)) looks like this:
```python
import transformers

import dllm

model_args, data_args, training_args = parser.parse_args_into_dataclasses()
# ----- Model ------------------------------------------------------------------
model = dllm.utils.get_model(model_args=model_args)
# ----- Tokenizer --------------------------------------------------------------
tokenizer = dllm.utils.get_tokenizer(model_args=model_args)
# ----- Dataset ----------------------------------------------------------------
dataset = "..."

# ----- Training --------------------------------------------------------------
trainer = dllm.core.trainers.MDLMTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    args=training_args,
    data_collator=transformers.DataCollatorForSeq2Seq(
        tokenizer,
        return_tensors="pt",
        padding=True,
        label_pad_token_id=tokenizer.pad_token_id, 
    ),
)
trainer.train()
```

You can launch training job locally with `accelerate`, or submit it to a [Slurm](https://slurm.schedmd.com/) cluster using `sbatch`.
```shell
# Run locally (ZeRO-2 on 8 GPUs with 4bit quantization and LoRA)
accelerate launch \
    --config_file scripts/accelerate_configs/zero2.yaml \
    examples/llada/sft.py \
    --num_train_epochs 4 \
    --load_in_4bit True --lora True
```
```shell
# Submit to a Slurm cluster (FSDP on 1 node, 8 GPUs)
sbatch --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "fsdp" \
    --script_path "examples/llada/sft.py" \
    --num_train_epochs 4

# Submit to a Slurm cluster (FSDP on 2 nodes, 16 GPUs)
sbatch --nodes=2 --gres=gpu:8 scripts/train.slurm.sh \
    --accelerate_config "fsdp" \
    --script_path "examples/llada/sft.py" \
    --num_train_epochs 4
```
See [Features & Documentation](#features--documentations) for training/inference details and task-specific recipes.


## Roadmap

- [ ] Support for additional diffusion LLMs.  

- [ ] Support for evaluation.

- [ ] Support for RL finetuning.


## Citation
```
@misc{dllm,
    author = {Zhanhui Zhou and Lingjie Chen and Hanghang Tong and Dawn Song},
    title = {dLLM: Simple Diffusion Language Modeling},
    year = {2025},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/ZHZisZZ/dllm}},
}
```
