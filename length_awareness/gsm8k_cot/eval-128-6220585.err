The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:13:32:22 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:22 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:22 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:22 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:22 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:22 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:32:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:32:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:32:22.499511446 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:32:22.499510008 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:32:22.499510102 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:23 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:23 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:23 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:23 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:23 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:23 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:23 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:23 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:32:23 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:23 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:32:23.763423460 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:23 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:23 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:32:23.767684127 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:23 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
[W1110 13:32:23.771764701 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:23 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:23 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:23 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:32:23.789669017 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:23 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:32:23 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:32:23 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:32:23 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:32:23.906915738 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:23 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.03it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.11it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.10s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.21s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.21s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.18s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.19s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.27s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.19s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.31s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]

Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]
[rank4]:[W1110 13:32:35.686795523 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 13:32:35.686887364 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 13:32:35.694313105 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 13:32:35.694488725 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 13:32:35.694648991 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 13:32:35.694706673 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 13:32:35.694852259 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 13:32:35.695405872 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:32:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.22it/s]100%|██████████| 10/10 [00:00<00:00, 86.83it/s]
2025-11-10:13:32:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:32:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.26it/s]100%|██████████| 10/10 [00:00<00:00, 86.06it/s]
 90%|█████████ | 9/10 [00:00<00:00, 87.35it/s]100%|██████████| 10/10 [00:00<00:00, 86.82it/s]
2025-11-10:13:32:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.98it/s]100%|██████████| 10/10 [00:00<00:00, 85.66it/s]
2025-11-10:13:32:42 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:42 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:42 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:42 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 84.21it/s]100%|██████████| 10/10 [00:00<00:00, 83.92it/s]
2025-11-10:13:32:42 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:42 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:42 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:42 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.37it/s]100%|██████████| 10/10 [00:00<00:00, 87.02it/s]
2025-11-10:13:32:42 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:42 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:42 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:42 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.53it/s]100%|██████████| 10/10 [00:00<00:00, 86.14it/s]
2025-11-10:13:32:43 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:32:43 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:32:43 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:32:43 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.06it/s]100%|██████████| 10/10 [00:00<00:00, 86.59it/s]
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:32:43 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f4dfc3db130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f4dfc3db130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f15000d7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f15000d7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fed1c11f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fed1c11f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa39c03b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa39c03b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fb300157130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fb300157130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa914d6f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa914d6f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc28c107130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc28c107130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f76b0167130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:32:46 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f76b0167130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 467.08 examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 435.17 examples/s]

Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 444.03 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 442.01 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 473.17 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 465.73 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 444.70 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 458.00 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.24s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.23s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.24s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.24s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.24s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.24s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.23s/it]Generating...:  10%|█         | 1/10 [00:13<01:59, 13.23s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.65s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.65s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.65s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.64s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.64s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.65s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.65s/it]Generating...:  20%|██        | 2/10 [00:25<01:41, 12.64s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.39s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  30%|███       | 3/10 [00:37<01:26, 12.40s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  40%|████      | 4/10 [00:49<01:13, 12.31s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.28s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.27s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.27s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.28s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.27s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.27s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.28s/it]Generating...:  50%|█████     | 5/10 [01:01<01:01, 12.27s/it]Generating...:  60%|██████    | 6/10 [01:14<00:48, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:49, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:48, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:48, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:49, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:49, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:48, 12.25s/it]Generating...:  60%|██████    | 6/10 [01:14<00:48, 12.25s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  70%|███████   | 7/10 [01:26<00:36, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  80%|████████  | 8/10 [01:38<00:24, 12.21s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...:  90%|█████████ | 9/10 [01:50<00:12, 12.22s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.23s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]Generating...: 100%|██████████| 10/10 [02:02<00:00, 12.30s/it]







[rank2]:W1110 13:34:49.946000 13787 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 46004 hash value: 54284289567294515
[rank0]:W1110 13:34:49.944000 13785 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43997 hash value: 13765344827789797353
[rank1]:W1110 13:34:49.945000 13786 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 45068 hash value: 3779835869189980357
[rank5]:W1110 13:34:49.947000 13790 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 46481 hash value: 4424698279881269368
[rank3]:W1110 13:34:49.944000 13788 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44747 hash value: 17794110457839701984
[rank4]:W1110 13:34:49.945000 13789 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 46187 hash value: 2271506377194326843
[rank6]:W1110 13:34:49.945000 13791 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 45397 hash value: 14297905586000368629
[rank7]:W1110 13:34:49.946000 13792 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 46470 hash value: 8724991585384974114
[rank0]:W1110 13:34:50.169000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.172000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.174000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank4]:W1110 13:34:50.174000 13789 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1388213327367728961
[rank3]:W1110 13:34:50.174000 13788 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12811382083187500602
[rank2]:W1110 13:34:50.174000 13787 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 242796673245879148
[rank5]:W1110 13:34:50.174000 13790 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13015660740567975991
[rank1]:W1110 13:34:50.174000 13786 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14001528413135027494
[rank7]:W1110 13:34:50.174000 13792 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16435863163039385061
[rank6]:W1110 13:34:50.174000 13791 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5669689752809136485
[rank0]:W1110 13:34:50.177000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.180000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.182000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.184000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.186000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 46481 hash value: 6571023869096268761
[rank0]:W1110 13:34:50.187000 13785 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1904811040713405961
[rank0]:W1110 13:34:50.192000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank3]:W1110 13:34:50.192000 13788 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 4489456075840039069
[rank4]:W1110 13:34:50.192000 13789 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7543514444797906921
[rank2]:W1110 13:34:50.192000 13787 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11842971219116748273
[rank5]:W1110 13:34:50.192000 13790 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 15707123182535767343
[rank6]:W1110 13:34:50.192000 13791 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9978332391599765918
[rank1]:W1110 13:34:50.192000 13786 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 6622625160992102594
[rank7]:W1110 13:34:50.192000 13792 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 10001847987461898319
[rank0]:W1110 13:34:50.193000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.197000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.198000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.198000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.199000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.200000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.201000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7888553014390979245
[rank0]:W1110 13:34:50.202000 13785 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11529454945357740130
[rank0]:W1110 13:34:50.206000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.207000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.208000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.209000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.210000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.211000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.212000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
[rank0]:W1110 13:34:50.213000 13785 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 3575399463418225692
2025-11-10:13:34:57 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:13:34:57 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1110 13:34:58.214438453 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
