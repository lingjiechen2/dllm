The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:13:30:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:21.247054479 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:30:21.247054684 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:30:21.247056694 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:22 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:22 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:22.625879916 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:30:22.626172858 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:22 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
[W1110 13:30:22.637562072 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:22 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:22.655025586 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:22 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:22 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:22 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:22 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:22.676722240 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:22 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.01s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.05s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.02s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.06s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.06s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.06s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.04s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.03s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.41s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.43s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.44s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.46s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.44s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.45s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.44s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.45s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.59s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.60s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.63s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.63s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.61s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.62s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.63s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.62s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.61s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.63s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.65s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.64s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.65s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.65s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.64s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.65s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.61s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.62s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.65s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.46s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.40s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.39s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.47s/it]


[rank7]:[W1110 13:30:41.156666179 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 13:30:41.156712979 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 13:30:41.156713313 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 13:30:41.156881677 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 13:30:41.156979527 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 13:30:41.157154103 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 13:30:41.157189252 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 13:30:41.159040067 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:47 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:47 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:47 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:47 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:30:47 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:47 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:47 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:47 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:30:47 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:47 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:47 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:47 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.65it/s]100%|██████████| 10/10 [00:00<00:00, 85.40it/s]
2025-11-10:13:30:47 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:47 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:47 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:47 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.08it/s]100%|██████████| 10/10 [00:00<00:00, 84.91it/s]
2025-11-10:13:30:48 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:48 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:48 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:48 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.14it/s]100%|██████████| 10/10 [00:00<00:00, 85.90it/s]
 90%|█████████ | 9/10 [00:00<00:00, 85.29it/s]100%|██████████| 10/10 [00:00<00:00, 85.06it/s]
 90%|█████████ | 9/10 [00:00<00:00, 86.28it/s]100%|██████████| 10/10 [00:00<00:00, 85.94it/s]
2025-11-10:13:30:48 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:48 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:48 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:48 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:30:48 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:48 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:48 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:48 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.46it/s]100%|██████████| 10/10 [00:00<00:00, 86.21it/s]
 90%|█████████ | 9/10 [00:00<00:00, 85.98it/s]100%|██████████| 10/10 [00:00<00:00, 85.68it/s]
2025-11-10:13:30:48 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:48 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:48 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:48 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.32it/s]100%|██████████| 10/10 [00:00<00:00, 86.90it/s]
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:48 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f6430037130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f6430037130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0e50117130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0e50117130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f19b018b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f19b018b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f080c237130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f080c237130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f537eaf7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f537eaf7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f094415f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f094415f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f01e012f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f01e012f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe88c3f3130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:51 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe88c3f3130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 445.50 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 467.79 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 468.40 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 449.97 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 331.44 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 450.45 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 397.01 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 339.23 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:06<01:01,  6.89s/it]Generating...:  10%|█         | 1/10 [00:06<01:02,  6.90s/it]Generating...:  10%|█         | 1/10 [00:06<01:02,  6.91s/it]Generating...:  10%|█         | 1/10 [00:06<01:02,  6.90s/it]Generating...:  10%|█         | 1/10 [00:06<01:01,  6.89s/it]Generating...:  10%|█         | 1/10 [00:06<01:01,  6.89s/it]Generating...:  10%|█         | 1/10 [00:06<01:02,  6.89s/it]Generating...:  10%|█         | 1/10 [00:06<01:01,  6.88s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.31s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.31s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.30s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.31s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.30s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.31s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.31s/it]Generating...:  20%|██        | 2/10 [00:12<00:50,  6.30s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  30%|███       | 3/10 [00:18<00:42,  6.08s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.98s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.97s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.97s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.97s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.97s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.98s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.97s/it]Generating...:  40%|████      | 4/10 [00:24<00:35,  5.98s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  50%|█████     | 5/10 [00:30<00:29,  5.92s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  60%|██████    | 6/10 [00:36<00:23,  5.94s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  70%|███████   | 7/10 [00:42<00:17,  5.92s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  80%|████████  | 8/10 [00:47<00:11,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...:  90%|█████████ | 9/10 [00:53<00:05,  5.89s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.92s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]Generating...: 100%|██████████| 10/10 [00:59<00:00,  5.98s/it]







[rank5]:W1110 13:31:51.384000 97716 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44523 hash value: 15979127879463394813
[rank2]:W1110 13:31:51.384000 97713 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44161 hash value: 5038195784568799020
[rank0]:W1110 13:31:51.384000 97711 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 41993 hash value: 2402907635492145359
[rank6]:W1110 13:31:51.383000 97717 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43464 hash value: 14495653508923923563
[rank3]:W1110 13:31:51.383000 97714 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 42766 hash value: 16661659457808274494
[rank1]:W1110 13:31:51.384000 97712 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43042 hash value: 8362589125581819253
[rank7]:W1110 13:31:51.383000 97718 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44555 hash value: 10399716537421782498
[rank4]:W1110 13:31:51.383000 97715 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44174 hash value: 207823616365434801
[rank0]:W1110 13:31:51.590000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.592000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.594000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank1]:W1110 13:31:51.596000 97712 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9839325898139593714
[rank6]:W1110 13:31:51.596000 97717 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16806950184955072125
[rank7]:W1110 13:31:51.596000 97718 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16435863163039385061
[rank4]:W1110 13:31:51.596000 97715 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9839325898139593714
[rank3]:W1110 13:31:51.596000 97714 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1063769190724560482
[rank2]:W1110 13:31:51.596000 97713 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7063068954892600916
[rank5]:W1110 13:31:51.596000 97716 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 4070057026342301240
[rank0]:W1110 13:31:51.596000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.601000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.603000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.605000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.607000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44555 hash value: 8450442538950735294
[rank0]:W1110 13:31:51.609000 97711 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 2866259879057004588
[rank0]:W1110 13:31:51.613000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank6]:W1110 13:31:51.613000 97717 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 2466787493227024220
[rank2]:W1110 13:31:51.613000 97713 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13196979716020587437
[rank1]:W1110 13:31:51.613000 97712 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9716308223032835091
[rank7]:W1110 13:31:51.613000 97718 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 10001847987461898319
[rank3]:W1110 13:31:51.613000 97714 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 10551206401673406509
[rank5]:W1110 13:31:51.613000 97716 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11090555702699821886
[rank4]:W1110 13:31:51.613000 97715 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14001528413135027494
[rank0]:W1110 13:31:51.614000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.618000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.619000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.620000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.621000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.622000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.623000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 15346003339155475969
[rank0]:W1110 13:31:51.624000 97711 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 3334910641039867152
[rank0]:W1110 13:31:51.628000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.629000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.630000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.631000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.632000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.633000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.634000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
[rank0]:W1110 13:31:51.635000 97711 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 12139451675016624156
2025-11-10:13:31:59 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:13:31:59 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1110 13:31:59.598742633 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
