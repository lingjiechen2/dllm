The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:13:30:05 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:05 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:05 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:05 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:05 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:05 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:05 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:05 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:05.568473736 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:30:05.568511730 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:05 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:06 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:06 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:06 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:06 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:06 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:06.691250599 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:06 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:06 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:06 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:06 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:06.703253968 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:06 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:06 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:06 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:06 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
[W1110 13:30:06.713914616 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:06 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:06 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:30:06 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:06 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:06 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:30:06 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
[W1110 13:30:06.728900560 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:06 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:06 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:13:30:06 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:30:06 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 32, 'steps': 32, 'block_length': 32, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 13:30:06.741968242 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:30:06.743398311 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:06 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:11,  2.33s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.41s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.45s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.44s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.46s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.46s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:11,  2.39s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.46s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:10,  2.63s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.83s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.84s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.86s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.81s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.83s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.85s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:05<00:11,  2.83s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.60s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.65s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.65s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:08,  2.70s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:08<00:07,  2.66s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:08<00:08,  2.67s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.66s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:08<00:07,  2.66s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.75s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.94s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.92s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.91s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.91s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.91s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.92s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:11<00:05,  2.91s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.65s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.62s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.65s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.65s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.65s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.67s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:13<00:02,  2.65s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.46s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.59s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.47s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.59s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.53s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.49s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.49s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.50s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]

Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]

[rank1]:[W1110 13:30:26.208312272 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 13:30:26.209631493 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 13:30:26.209935324 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 13:30:26.210734526 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 13:30:26.210974662 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 13:30:26.210976599 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 13:30:26.211212540 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 13:30:26.211367938 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:30:32 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:32 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:32 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:32 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:30:33 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:33 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:33 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:33 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.61it/s]100%|██████████| 10/10 [00:00<00:00, 85.14it/s]
 90%|█████████ | 9/10 [00:00<00:00, 85.75it/s]2025-11-10:13:30:33 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:33 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
100%|██████████| 10/10 [00:00<00:00, 84.78it/s]2025-11-10:13:30:33 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.

2025-11-10:13:30:33 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:30:33 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:33 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:33 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:33 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.17it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.08it/s]100%|██████████| 10/10 [00:00<00:00, 85.71it/s]
100%|██████████| 10/10 [00:00<00:00, 86.63it/s]
2025-11-10:13:30:33 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:33 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:33 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:33 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 84.48it/s]100%|██████████| 10/10 [00:00<00:00, 84.22it/s]
2025-11-10:13:30:33 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:33 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:33 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:33 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:30:33 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:33 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:33 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:33 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.95it/s]100%|██████████| 10/10 [00:00<00:00, 86.57it/s]
 90%|█████████ | 9/10 [00:00<00:00, 86.65it/s]100%|██████████| 10/10 [00:00<00:00, 86.14it/s]
2025-11-10:13:30:34 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:30:34 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:30:34 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:30:34 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.49it/s]100%|██████████| 10/10 [00:00<00:00, 86.10it/s]
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:30:34 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f66a02af130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f66a02af130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fadac107130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fadac107130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f4072607130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f4072607130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe5000b7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe5000b7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f9323b33130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f9323b33130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc5f0133130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc5f0133130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fda500fb130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe019d33130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fda500fb130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:30:37 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe019d33130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 441.96 examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 449.64 examples/s]

Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 469.84 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 451.69 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 462.54 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 475.45 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 484.89 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 443.50 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.07s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.08s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.09s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.08s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.07s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.08s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.09s/it]Generating...:  10%|█         | 1/10 [00:05<00:45,  5.08s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.77s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.77s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.77s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.78s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.77s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.78s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.78s/it]Generating...:  20%|██        | 2/10 [00:07<00:30,  3.77s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.33s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.33s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.33s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.34s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.34s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.34s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.34s/it]Generating...:  30%|███       | 3/10 [00:10<00:23,  3.34s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.13s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.14s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.13s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.13s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.14s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.13s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.13s/it]Generating...:  40%|████      | 4/10 [00:13<00:18,  3.14s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.03s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.03s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.04s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.03s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.04s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.04s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.03s/it]Generating...:  50%|█████     | 5/10 [00:16<00:15,  3.03s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  60%|██████    | 6/10 [00:19<00:11,  2.98s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  70%|███████   | 7/10 [00:22<00:08,  2.94s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  80%|████████  | 8/10 [00:25<00:05,  2.92s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...:  90%|█████████ | 9/10 [00:27<00:02,  2.91s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  2.90s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]Generating...: 100%|██████████| 10/10 [00:30<00:00,  3.08s/it]







[rank7]:W1110 13:31:08.191000 5606 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43646 hash value: 10846620640639522560
[rank4]:W1110 13:31:08.191000 5603 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43254 hash value: 1051304915416067106
[rank6]:W1110 13:31:08.191000 5605 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 42655 hash value: 14349125335282921338
[rank1]:W1110 13:31:08.191000 5599 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 42298 hash value: 13673023357377015558
[rank5]:W1110 13:31:08.193000 5604 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43589 hash value: 2458813767049045831
[rank2]:W1110 13:31:08.191000 5600 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43409 hash value: 7130449461498175891
[rank0]:W1110 13:31:08.191000 5598 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40955 hash value: 4994295386747178483
[rank3]:W1110 13:31:08.191000 5602 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 41812 hash value: 16877069725124380030
[rank0]:W1110 13:31:08.417000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.420000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.422000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.424000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank4]:W1110 13:31:08.426000 5603 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 2558427400426605189
[rank2]:W1110 13:31:08.426000 5600 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 3531483430220052286
[rank1]:W1110 13:31:08.426000 5599 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12733329051678944693
[rank5]:W1110 13:31:08.426000 5604 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 4083341601733307283
[rank7]:W1110 13:31:08.426000 5606 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16435863163039385061
[rank3]:W1110 13:31:08.426000 5602 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9706480393622568084
[rank6]:W1110 13:31:08.426000 5605 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16336006871261796623
[rank0]:W1110 13:31:08.426000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.431000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.434000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.436000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 43646 hash value: 771740629759188182
[rank0]:W1110 13:31:08.437000 5598 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12710773063978892428
[rank0]:W1110 13:31:08.442000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank7]:W1110 13:31:08.442000 5606 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 6865792535231927789
[rank1]:W1110 13:31:08.442000 5599 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12733329051678944693
[rank0]:W1110 13:31:08.443000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank4]:W1110 13:31:08.442000 5603 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 2558427400426605189
[rank5]:W1110 13:31:08.442000 5604 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7140253769222256854
[rank3]:W1110 13:31:08.442000 5602 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9706480393622568084
[rank2]:W1110 13:31:08.442000 5600 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 3531483430220052286
[rank6]:W1110 13:31:08.442000 5605 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16336006871261796623
[rank0]:W1110 13:31:08.445000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank0]:W1110 13:31:08.447000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank0]:W1110 13:31:08.449000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank0]:W1110 13:31:08.450000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank0]:W1110 13:31:08.451000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank0]:W1110 13:31:08.452000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 16975035192308913341
[rank0]:W1110 13:31:08.453000 5598 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 6961436697482013072
[rank0]:W1110 13:31:08.457000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.458000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.459000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.461000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.462000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.463000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.464000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
[rank0]:W1110 13:31:08.465000 5598 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 7903205422599050867
2025-11-10:13:31:16 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:13:31:16 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1110 13:31:17.349246510 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
