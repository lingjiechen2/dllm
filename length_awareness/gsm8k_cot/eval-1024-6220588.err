The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:36:01.826062551 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:36:01.826069978 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:36:01.826093393 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:36:01.329204407 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:36:01.330137106 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:36:01.331728341 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:36:01.342206656 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:36:01 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:36:01 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:36:01 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:36:01 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:36:01.522998505 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:36:02 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.02it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.04it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.04it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.30s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.17s/it]


Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.15s/it]



Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]
[rank0]:[W1110 13:36:13.234671810 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 13:36:13.235944779 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 13:36:13.238782551 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 13:36:13.239023246 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 13:36:13.239031327 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 13:36:13.239132791 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 13:36:13.239141257 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 13:36:13.239932679 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:36:19 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:19 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:19 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:19 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:36:20 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:20 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:20 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.50it/s]100%|██████████| 10/10 [00:00<00:00, 87.05it/s]
 90%|█████████ | 9/10 [00:00<00:00, 85.90it/s]2025-11-10:13:36:20 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:20 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:20 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 85.46it/s]
2025-11-10:13:36:20 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:20 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:20 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.58it/s]100%|██████████| 10/10 [00:00<00:00, 86.19it/s]
2025-11-10:13:36:20 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:20 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:20 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.86it/s]100%|██████████| 10/10 [00:00<00:00, 86.58it/s]
 90%|█████████ | 9/10 [00:00<00:00, 86.05it/s]100%|██████████| 10/10 [00:00<00:00, 85.62it/s]
2025-11-10:13:36:20 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:20 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:20 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:20 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:20 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:20 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 5...
2025-11-10:13:36:20 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 88.40it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.66it/s]100%|██████████| 10/10 [00:00<00:00, 87.88it/s]
100%|██████████| 10/10 [00:00<00:00, 87.24it/s]
2025-11-10:13:36:21 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:36:21 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:36:21 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:36:21 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.63it/s]100%|██████████| 10/10 [00:00<00:00, 87.22it/s]
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:36:21 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f23ebb03130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f23ebb03130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f49a00c7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f49a00c7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f64b0133130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f64b0133130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 448.04 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0bbc123130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0bbc123130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7feab0193130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7feab0193130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5306557130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5306557130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|██████████| 10/10 [00:00<00:00, 462.08 examples/s]
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f1690377130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f1690377130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f474c073130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:36:24 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f474c073130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 459.96 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 476.50 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 476.07 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 443.74 examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 480.39 examples/s]

Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 302.94 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.62s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.64s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.63s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.65s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.62s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.63s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.62s/it]Generating...:  10%|█         | 1/10 [03:05<27:50, 185.60s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.07s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.07s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.07s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.08s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.06s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.06s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.06s/it]Generating...:  20%|██        | 2/10 [06:10<24:40, 185.07s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.26s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  30%|███       | 3/10 [09:13<21:29, 184.25s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  40%|████      | 4/10 [12:16<18:23, 183.88s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  50%|█████     | 5/10 [15:20<15:19, 183.83s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  60%|██████    | 6/10 [18:24<12:14, 183.69s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  70%|███████   | 7/10 [21:27<09:10, 183.50s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  80%|████████  | 8/10 [24:31<06:07, 183.62s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:34<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...:  90%|█████████ | 9/10 [27:35<03:03, 183.73s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.77s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]
Generating...: 100%|██████████| 10/10 [30:38<00:00, 183.89s/it]






[rank1]:W1110 14:07:03.091000 20806 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48633 hash value: 16710680391578948466
[rank0]:W1110 14:07:03.092000 20805 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44814 hash value: 4469181417368520220
[rank4]:W1110 14:07:03.091000 20809 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50639 hash value: 705568663325774390
[rank2]:W1110 14:07:03.092000 20807 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48583 hash value: 6066050767414517408
[rank3]:W1110 14:07:03.091000 20808 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 47081 hash value: 11105243234380517891
[rank5]:W1110 14:07:03.092000 20810 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50534 hash value: 9501370331926192762
[rank6]:W1110 14:07:03.091000 20811 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48796 hash value: 1359078789712485629
[rank7]:W1110 14:07:03.091000 20812 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49201 hash value: 8216769152033155952
[rank0]:W1110 14:07:03.299000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.302000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.304000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank2]:W1110 14:07:03.306000 20807 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 18171203510859033245
[rank4]:W1110 14:07:03.306000 20809 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 8882603949968533886
[rank7]:W1110 14:07:03.306000 20812 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 763888721488103861
[rank1]:W1110 14:07:03.306000 20806 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7543514444797906921
[rank3]:W1110 14:07:03.306000 20808 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 246355492444223445
[rank6]:W1110 14:07:03.306000 20811 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14954965928495158303
[rank5]:W1110 14:07:03.306000 20810 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 8996483710269076226
[rank0]:W1110 14:07:03.307000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.312000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.315000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.317000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.319000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50639 hash value: 5682531420360728833
[rank0]:W1110 14:07:03.320000 20805 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1904811040713405961
[rank0]:W1110 14:07:03.325000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank7]:W1110 14:07:03.325000 20812 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13408921249447924636
[rank4]:W1110 14:07:03.325000 20809 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 8882603949968533886
[rank6]:W1110 14:07:03.325000 20811 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9274809806336136030
[rank3]:W1110 14:07:03.325000 20808 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13266156194351881422
[rank2]:W1110 14:07:03.325000 20807 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13987721871498389887
[rank5]:W1110 14:07:03.325000 20810 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 17086732441681874330
[rank1]:W1110 14:07:03.325000 20806 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13966572686153119721
[rank0]:W1110 14:07:03.327000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.330000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.331000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.332000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.333000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.334000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.335000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 1615799744144632534
[rank0]:W1110 14:07:03.336000 20805 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11529454945357740130
[rank0]:W1110 14:07:03.340000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.342000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.343000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.344000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.345000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.346000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.347000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
[rank0]:W1110 14:07:03.347000 20805 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9410464512467698404
2025-11-10:14:07:10 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:14:07:10 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1110 14:07:11.308290964 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
