The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:31.138654744 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:31.138662223 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:31.138664744 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:31.527306021 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:31.529267389 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:31.530599644 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:31.557559970 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:31 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:31 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:31 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:31 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:31.689495581 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:31 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:09,  1.94s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:09,  1.99s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:09,  2.00s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:09,  1.99s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.01s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.03s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.01s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:10,  2.01s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.36s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.40s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.41s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.40s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.39s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.42s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.40s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:04<00:09,  2.40s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.53s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.55s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.57s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.56s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:07<00:07,  2.57s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.57s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.60s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.61s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.60s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.60s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.60s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.60s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:10<00:05,  2.60s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.60s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.62s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.63s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.64s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.63s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.63s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:12<00:02,  2.63s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.36s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.37s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.38s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.44s/it]


Loading checkpoint shards: 100%|██████████| 6/6 [00:14<00:00,  2.45s/it]
[rank3]:[W1110 13:33:51.022700187 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 13:33:51.023150333 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 13:33:51.023640128 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 13:33:51.023693469 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 13:33:51.023828312 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 13:33:51.023851303 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 13:33:51.023951233 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 13:33:51.024565348 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:57 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:57 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:57 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:57 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:33:58 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:58 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:58 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:58 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.69it/s]100%|██████████| 10/10 [00:00<00:00, 86.38it/s]
 90%|█████████ | 9/10 [00:00<00:00, 86.13it/s]100%|██████████| 10/10 [00:00<00:00, 85.71it/s]
2025-11-10:13:33:58 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:58 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:58 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:58 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.90it/s]100%|██████████| 10/10 [00:00<00:00, 86.36it/s]
2025-11-10:13:33:59 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:59 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:59 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:59 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.57it/s]100%|██████████| 10/10 [00:00<00:00, 87.26it/s]
2025-11-10:13:33:59 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:59 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:59 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:59 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.06it/s]100%|██████████| 10/10 [00:00<00:00, 86.71it/s]
2025-11-10:13:34:00 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:34:00 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:34:00 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:34:00 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.66it/s]100%|██████████| 10/10 [00:00<00:00, 87.33it/s]
2025-11-10:13:34:01 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:34:01 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:34:01 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:34:01 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.11it/s]100%|██████████| 10/10 [00:00<00:00, 84.76it/s]
2025-11-10:13:34:01 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:34:01 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:34:01 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:34:01 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.06it/s]100%|██████████| 10/10 [00:00<00:00, 86.81it/s]
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:34:01 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fb591d53130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fb591d53130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc5002bf130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc5002bf130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f49102a7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f49102a7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f4dc02db130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f4dc02db130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe6d0153130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe6d0153130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 468.32 examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 437.54 examples/s]

Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f889c17b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f889c17b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f71e011f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|██████████| 10/10 [00:00<00:00, 457.93 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f71e011f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f66927d7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

2025-11-10:13:34:04 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f66927d7130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 481.49 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 483.70 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 486.59 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 484.90 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 479.67 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.42s/it]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.44s/it]Generating...:  10%|█         | 1/10 [01:06<09:58, 66.44s/it]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.42s/it]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.44s/it]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.43s/it]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.43s/it]Generating...:  10%|█         | 1/10 [01:06<09:57, 66.42s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.22s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.22s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.22s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.23s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.23s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.22s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.22s/it]Generating...:  20%|██        | 2/10 [02:12<08:49, 66.23s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.82s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.82s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.82s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.83s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.83s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.82s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.83s/it]Generating...:  30%|███       | 3/10 [03:17<07:40, 65.82s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  40%|████      | 4/10 [04:23<06:33, 65.58s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  50%|█████     | 5/10 [05:28<05:27, 65.51s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.46s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.46s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.46s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.47s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.47s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.47s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.47s/it]Generating...:  60%|██████    | 6/10 [06:33<04:21, 65.47s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  70%|███████   | 7/10 [07:38<03:16, 65.34s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  80%|████████  | 8/10 [08:44<02:10, 65.41s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...:  90%|█████████ | 9/10 [09:50<01:05, 65.54s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.52s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]Generating...: 100%|██████████| 10/10 [10:55<00:00, 65.58s/it]







[rank4]:W1110 13:45:00.256000 82250 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 52825 hash value: 431571900261257426
[rank2]:W1110 13:45:00.256000 82248 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49011 hash value: 2732001480846279122
[rank1]:W1110 13:45:00.256000 82247 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50268 hash value: 16539221529059810805
[rank5]:W1110 13:45:00.255000 82251 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 52651 hash value: 2502346841570908978
[rank6]:W1110 13:45:00.256000 82252 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 51657 hash value: 1663323619141677491
[rank7]:W1110 13:45:00.256000 82253 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 52088 hash value: 11711625197934992651
[rank3]:W1110 13:45:00.256000 82249 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49240 hash value: 8259538645014760163
[rank0]:W1110 13:45:00.256000 82245 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 45354 hash value: 18192211324167774569
[rank0]:W1110 13:45:00.467000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank0]:W1110 13:45:00.469000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank0]:W1110 13:45:00.472000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank2]:W1110 13:45:00.473000 82248 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9907810848062296281
[rank4]:W1110 13:45:00.473000 82250 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14338441847616608035
[rank5]:W1110 13:45:00.473000 82251 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 8996483710269076226
[rank0]:W1110 13:45:00.474000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank3]:W1110 13:45:00.473000 82249 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13106863238146753490
[rank1]:W1110 13:45:00.473000 82247 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11534632651634190984
[rank6]:W1110 13:45:00.473000 82252 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16449850352743480063
[rank7]:W1110 13:45:00.474000 82253 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16435863163039385061
[rank0]:W1110 13:45:00.478000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank0]:W1110 13:45:00.480000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank0]:W1110 13:45:00.482000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank0]:W1110 13:45:00.484000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 52825 hash value: 13130799488006808833
[rank0]:W1110 13:45:00.485000 82245 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1904811040713405961
[rank0]:W1110 13:45:00.490000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank7]:W1110 13:45:00.490000 82253 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7304184267884195655
[rank6]:W1110 13:45:00.490000 82252 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 2502845835185590391
[rank4]:W1110 13:45:00.490000 82250 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14338441847616608035
[rank3]:W1110 13:45:00.490000 82249 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12964453425694940751
[rank5]:W1110 13:45:00.490000 82251 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9862380554579330185
[rank1]:W1110 13:45:00.490000 82247 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 185517005202678646
[rank2]:W1110 13:45:00.490000 82248 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13987721871498389887
[rank0]:W1110 13:45:00.491000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.495000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.496000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.497000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.498000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.499000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.499000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 556711652064414244
[rank0]:W1110 13:45:00.501000 82245 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11529454945357740130
[rank0]:W1110 13:45:00.505000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.506000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.507000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.508000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.509000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.510000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.511000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
[rank0]:W1110 13:45:00.512000 82245 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 2753889213498319799
2025-11-10:13:45:07 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:13:45:07 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1110 13:45:08.693881535 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
