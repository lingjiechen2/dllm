The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-13:03:17:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:17:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:17:16 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-13:03:17:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:17:16 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-13:03:17:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:17:16 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-13:03:17:16 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-13:03:17:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:17:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:17:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:17:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:17:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:17:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:17:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:17:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1113 03:17:16.575184048 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:17:16.575184332 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:17:16.575186883 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:17:16.575187798 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-13:03:17:17 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.13it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:04,  1.00s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.02s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:04,  1.02s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.03s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.09s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.09s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.13s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.08s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.12s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.12s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.13s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.11s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.13s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.13s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it]
[rank1]:[W1113 03:17:26.120836674 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1113 03:17:26.121124410 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1113 03:17:26.121187024 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1113 03:17:26.121233271 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-13:03:17:32 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-13:03:17:32 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-13:03:17:32 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:17:32 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/20 [00:00<?, ?it/s]2025-11-13:03:17:32 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-13:03:17:32 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-13:03:17:32 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:17:32 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/20 [00:00<?, ?it/s]2025-11-13:03:17:32 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-13:03:17:32 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-13:03:17:32 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:17:32 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/20 [00:00<?, ?it/s]2025-11-13:03:17:32 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-13:03:17:32 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-13:03:17:32 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:17:32 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/20 [00:00<?, ?it/s] 30%|███       | 6/20 [00:00<00:00, 56.16it/s] 45%|████▌     | 9/20 [00:00<00:00, 86.98it/s] 45%|████▌     | 9/20 [00:00<00:00, 86.50it/s] 45%|████▌     | 9/20 [00:00<00:00, 87.50it/s] 65%|██████▌   | 13/20 [00:00<00:00, 61.44it/s] 95%|█████████▌| 19/20 [00:00<00:00, 88.71it/s] 90%|█████████ | 18/20 [00:00<00:00, 88.11it/s]100%|██████████| 20/20 [00:00<00:00, 88.30it/s]
100%|██████████| 20/20 [00:00<00:00, 87.64it/s]
 95%|█████████▌| 19/20 [00:00<00:00, 88.93it/s]100%|██████████| 20/20 [00:00<00:00, 68.02it/s]
100%|██████████| 20/20 [00:00<00:00, 88.42it/s]
2025-11-13:03:17:33 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:17:33 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:17:33 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:17:33 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe6c0163910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:17:35 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe6c0163910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f83d23c7910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/20 [00:00<?, ? examples/s]2025-11-13:03:17:35 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f83d23c7910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/20 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe709f47910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f15b82b3910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:17:35 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe709f47910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:17:35 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f15b82b3910> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/20 [00:00<?, ? examples/s]Map:   0%|          | 0/20 [00:00<?, ? examples/s]Map: 100%|██████████| 20/20 [00:00<00:00, 540.72 examples/s]Map: 100%|██████████| 20/20 [00:00<00:00, 521.24 examples/s]

Map: 100%|██████████| 20/20 [00:00<00:00, 545.83 examples/s]Generating...:   0%|          | 0/20 [00:00<?, ?it/s]Map: 100%|██████████| 20/20 [00:00<00:00, 543.17 examples/s]
Generating...:   0%|          | 0/20 [00:00<?, ?it/s]
Generating...:   0%|          | 0/20 [00:00<?, ?it/s]Generating...:   0%|          | 0/20 [00:00<?, ?it/s]Generating...:   5%|▌         | 1/20 [01:05<20:35, 65.02s/it]Generating...:   5%|▌         | 1/20 [01:05<20:35, 65.02s/it]Generating...:   5%|▌         | 1/20 [01:05<20:35, 65.03s/it]Generating...:   5%|▌         | 1/20 [01:05<20:35, 65.02s/it]Generating...:  10%|█         | 2/20 [02:10<19:38, 65.47s/it]Generating...:  10%|█         | 2/20 [02:10<19:38, 65.47s/it]Generating...:  10%|█         | 2/20 [02:10<19:38, 65.47s/it]Generating...:  10%|█         | 2/20 [02:10<19:38, 65.47s/it]Generating...:  15%|█▌        | 3/20 [03:16<18:33, 65.47s/it]Generating...:  15%|█▌        | 3/20 [03:16<18:33, 65.47s/it]Generating...:  15%|█▌        | 3/20 [03:16<18:33, 65.47s/it]Generating...:  15%|█▌        | 3/20 [03:16<18:33, 65.47s/it]Generating...:  20%|██        | 4/20 [04:21<17:25, 65.36s/it]Generating...:  20%|██        | 4/20 [04:21<17:25, 65.36s/it]Generating...:  20%|██        | 4/20 [04:21<17:25, 65.37s/it]Generating...:  20%|██        | 4/20 [04:21<17:25, 65.36s/it]Generating...:  25%|██▌       | 5/20 [05:25<16:15, 65.05s/it]Generating...:  25%|██▌       | 5/20 [05:25<16:15, 65.05s/it]Generating...:  25%|██▌       | 5/20 [05:25<16:15, 65.05s/it]Generating...:  25%|██▌       | 5/20 [05:25<16:15, 65.05s/it]Generating...:  30%|███       | 6/20 [06:31<15:11, 65.11s/it]Generating...:  30%|███       | 6/20 [06:31<15:11, 65.11s/it]Generating...:  30%|███       | 6/20 [06:31<15:11, 65.11s/it]Generating...:  30%|███       | 6/20 [06:31<15:11, 65.11s/it]Generating...:  35%|███▌      | 7/20 [07:35<14:05, 65.00s/it]Generating...:  35%|███▌      | 7/20 [07:35<14:05, 65.00s/it]Generating...:  35%|███▌      | 7/20 [07:35<14:05, 65.00s/it]Generating...:  35%|███▌      | 7/20 [07:35<14:05, 65.00s/it]Generating...:  40%|████      | 8/20 [08:40<12:57, 64.81s/it]Generating...:  40%|████      | 8/20 [08:40<12:57, 64.81s/it]Generating...:  40%|████      | 8/20 [08:40<12:57, 64.81s/it]Generating...:  40%|████      | 8/20 [08:40<12:57, 64.81s/it]Generating...:  45%|████▌     | 9/20 [10:25<14:12, 77.53s/it]Generating...:  45%|████▌     | 9/20 [10:25<14:12, 77.53s/it]Generating...:  45%|████▌     | 9/20 [10:25<14:12, 77.53s/it]Generating...:  45%|████▌     | 9/20 [10:25<14:12, 77.53s/it]Generating...:  50%|█████     | 10/20 [12:28<15:14, 91.43s/it]Generating...:  50%|█████     | 10/20 [12:28<15:14, 91.43s/it]Generating...:  50%|█████     | 10/20 [12:28<15:14, 91.43s/it]Generating...:  50%|█████     | 10/20 [12:28<15:14, 91.43s/it]Generating...:  55%|█████▌    | 11/20 [14:31<15:09, 101.10s/it]Generating...:  55%|█████▌    | 11/20 [14:31<15:09, 101.10s/it]Generating...:  55%|█████▌    | 11/20 [14:31<15:09, 101.10s/it]Generating...:  55%|█████▌    | 11/20 [14:31<15:09, 101.10s/it]Generating...:  60%|██████    | 12/20 [16:36<14:26, 108.33s/it]Generating...:  60%|██████    | 12/20 [16:36<14:26, 108.33s/it]Generating...:  60%|██████    | 12/20 [16:36<14:26, 108.33s/it]Generating...:  60%|██████    | 12/20 [16:36<14:26, 108.33s/it]Generating...:  65%|██████▌   | 13/20 [18:34<12:59, 111.34s/it]Generating...:  65%|██████▌   | 13/20 [18:34<12:59, 111.34s/it]Generating...:  65%|██████▌   | 13/20 [18:34<12:59, 111.34s/it]Generating...:  65%|██████▌   | 13/20 [18:34<12:59, 111.34s/it]Generating...:  70%|███████   | 14/20 [20:42<11:37, 116.21s/it]Generating...:  70%|███████   | 14/20 [20:42<11:37, 116.21s/it]Generating...:  70%|███████   | 14/20 [20:42<11:37, 116.21s/it]Generating...:  70%|███████   | 14/20 [20:42<11:37, 116.21s/it]Generating...:  75%|███████▌  | 15/20 [22:44<09:50, 118.10s/it]Generating...:  75%|███████▌  | 15/20 [22:44<09:50, 118.10s/it]Generating...:  75%|███████▌  | 15/20 [22:44<09:50, 118.10s/it]Generating...:  75%|███████▌  | 15/20 [22:44<09:50, 118.10s/it]Generating...:  80%|████████  | 16/20 [24:40<07:49, 117.38s/it]Generating...:  80%|████████  | 16/20 [24:40<07:49, 117.38s/it]Generating...:  80%|████████  | 16/20 [24:40<07:49, 117.38s/it]Generating...:  80%|████████  | 16/20 [24:40<07:49, 117.38s/it]Generating...:  85%|████████▌ | 17/20 [26:07<05:24, 108.18s/it]Generating...:  85%|████████▌ | 17/20 [26:07<05:24, 108.18s/it]Generating...:  85%|████████▌ | 17/20 [26:07<05:24, 108.18s/it]Generating...:  85%|████████▌ | 17/20 [26:07<05:24, 108.18s/it]Generating...:  90%|█████████ | 18/20 [27:11<03:10, 95.03s/it] Generating...:  90%|█████████ | 18/20 [27:11<03:10, 95.03s/it] Generating...:  90%|█████████ | 18/20 [27:11<03:10, 95.03s/it] Generating...:  90%|█████████ | 18/20 [27:11<03:10, 95.03s/it] Generating...:  95%|█████████▌| 19/20 [28:17<01:26, 86.29s/it]Generating...:  95%|█████████▌| 19/20 [28:17<01:26, 86.29s/it]Generating...:  95%|█████████▌| 19/20 [28:17<01:26, 86.29s/it]Generating...:  95%|█████████▌| 19/20 [28:17<01:26, 86.29s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 79.79s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 79.79s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 79.79s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 79.79s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 88.10s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 88.10s/it]
Generating...: 100%|██████████| 20/20 [29:22<00:00, 88.10s/it]Generating...: 100%|██████████| 20/20 [29:22<00:00, 88.10s/it]


[rank0]:W1113 03:46:58.092000 17270 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 97986 hash value: 115743597293460881
[rank2]:W1113 03:46:58.093000 17272 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 100440 hash value: 17474936373758819783
[rank3]:W1113 03:46:58.091000 17273 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 101108 hash value: 15495081053350495474
[rank1]:W1113 03:46:58.092000 17271 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 102685 hash value: 8898593519603985156
[rank0]:W1113 03:46:58.174000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 102685 hash value: 13870891591933881992
[rank0]:W1113 03:46:58.177000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 102685 hash value: 13870891591933881992
[rank0]:W1113 03:46:58.179000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 102685 hash value: 13870891591933881992
[rank0]:W1113 03:46:58.182000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 102685 hash value: 13870891591933881992
[rank2]:W1113 03:46:58.182000 17272 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 2749483155636179235
[rank3]:W1113 03:46:58.182000 17273 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 8729794608209735341
[rank1]:W1113 03:46:58.182000 17271 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 14822696969022821157
[rank0]:W1113 03:46:58.183000 17270 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 11420963934749558221
[rank0]:W1113 03:46:58.186000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 16374169492567580549
[rank2]:W1113 03:46:58.187000 17272 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 14428488581750047858
[rank1]:W1113 03:46:58.186000 17271 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 4342302309602401275
[rank3]:W1113 03:46:58.186000 17273 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 315316088468905836
[rank0]:W1113 03:46:58.187000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 16374169492567580549
[rank0]:W1113 03:46:58.189000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 16374169492567580549
[rank0]:W1113 03:46:58.190000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 16374169492567580549
[rank0]:W1113 03:46:58.191000 17270 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 481 hash value: 4967538611933147799
[rank0]:W1113 03:46:58.193000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 11081463962236380166
[rank0]:W1113 03:46:58.194000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 11081463962236380166
[rank0]:W1113 03:46:58.195000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 11081463962236380166
[rank0]:W1113 03:46:58.196000 17270 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 481 hash value: 11081463962236380166
2025-11-13:03:47:05 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-13:03:47:05 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1113 03:47:06.844755246 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
