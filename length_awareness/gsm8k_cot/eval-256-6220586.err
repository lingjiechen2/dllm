The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:21.859062977 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:21.859068555 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:21.859089721 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:21.175980060 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 13:33:21.176349418 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
[W1110 13:33:21.178018362 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:21.182963455 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:13:33:21 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:13:33:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:13:33:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 13:33:21.354403877 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:21 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.16it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.03it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.16s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.20s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.21s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.17s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.19s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.18s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.22s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.16s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.21s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.20s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.20s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.21s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.21s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.20s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]


Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]

Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.14s/it]
[rank3]:[W1110 13:33:33.714743634 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 13:33:33.752466618 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 13:33:33.752890455 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 13:33:33.766441276 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 13:33:33.766762077 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 13:33:33.766802767 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 13:33:33.767190899 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 13:33:33.767205599 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:13:33:39 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:39 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:39 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:39 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.86it/s]100%|██████████| 10/10 [00:00<00:00, 85.71it/s]
2025-11-10:13:33:40 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:40 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:40 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 84.26it/s]100%|██████████| 10/10 [00:00<00:00, 84.14it/s]
2025-11-10:13:33:40 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:40 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:40 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:33:40 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:40 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:40 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.58it/s]100%|██████████| 10/10 [00:00<00:00, 85.39it/s]
 90%|█████████ | 9/10 [00:00<00:00, 85.94it/s]100%|██████████| 10/10 [00:00<00:00, 85.51it/s]
2025-11-10:13:33:40 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:40 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:40 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 87.04it/s]100%|██████████| 10/10 [00:00<00:00, 86.66it/s]
2025-11-10:13:33:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:13:33:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 85.13it/s]100%|██████████| 10/10 [00:00<00:00, 84.88it/s]
 90%|█████████ | 9/10 [00:00<00:00, 85.84it/s]100%|██████████| 10/10 [00:00<00:00, 85.47it/s]
2025-11-10:13:33:41 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:13:33:41 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:13:33:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:13:33:41 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s] 90%|█████████ | 9/10 [00:00<00:00, 86.44it/s]100%|██████████| 10/10 [00:00<00:00, 86.18it/s]
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
2025-11-10:13:33:41 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f62f029f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f62f029f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2cc4093130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2cc4093130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe99c433130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe99c433130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f6f4418f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f6f4418f130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f69400bb130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f69400bb130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f27d006b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f27d006b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 418.43 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5b90083130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5b90083130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2b4022b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:13:33:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2b4022b130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 439.08 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 458.27 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 469.96 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 464.43 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 451.40 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 474.76 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 442.77 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.25s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.27s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.26s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.25s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.24s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.24s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.26s/it]Generating...:  10%|█         | 1/10 [00:27<04:05, 27.24s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.86s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.86s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.85s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.86s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.86s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.85s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.86s/it]Generating...:  20%|██        | 2/10 [00:53<03:34, 26.85s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.66s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.67s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.66s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.67s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.67s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.66s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.67s/it]Generating...:  30%|███       | 3/10 [01:20<03:06, 26.67s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  40%|████      | 4/10 [01:46<02:39, 26.55s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  50%|█████     | 5/10 [02:12<02:12, 26.47s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  60%|██████    | 6/10 [02:39<01:45, 26.48s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  70%|███████   | 7/10 [03:05<01:19, 26.42s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:32<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  80%|████████  | 8/10 [03:31<00:52, 26.36s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...:  90%|█████████ | 9/10 [03:58<00:26, 26.37s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.40s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.48s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.49s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.49s/it]
Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.49s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.48s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.48s/it]Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.48s/it]
Generating...: 100%|██████████| 10/10 [04:24<00:00, 26.49s/it]





[rank7]:W1110 13:38:09.532000 103733 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48420 hash value: 2565011919919068012
[rank1]:W1110 13:38:09.532000 103727 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48981 hash value: 15930895617867141863
[rank5]:W1110 13:38:09.532000 103731 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50700 hash value: 5708725275636132039
[rank2]:W1110 13:38:09.533000 103728 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49297 hash value: 13075703965533015985
[rank3]:W1110 13:38:09.532000 103729 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48016 hash value: 8675337194408477978
[rank4]:W1110 13:38:09.532000 103730 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50028 hash value: 4834665392376555858
[rank0]:W1110 13:38:09.532000 103726 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 47189 hash value: 10568747684018702741
[rank6]:W1110 13:38:09.533000 103732 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49241 hash value: 15504466867541969118
[rank0]:W1110 13:38:09.745000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.747000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.750000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.752000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank5]:W1110 13:38:09.752000 103731 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 3488499697021316065
[rank4]:W1110 13:38:09.752000 103730 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9839325898139593714
[rank7]:W1110 13:38:09.752000 103733 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16435863163039385061
[rank1]:W1110 13:38:09.752000 103727 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 6535673186639736711
[rank2]:W1110 13:38:09.752000 103728 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 242796673245879148
[rank3]:W1110 13:38:09.752000 103729 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 18349260030872487688
[rank6]:W1110 13:38:09.752000 103732 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5669689752809136485
[rank0]:W1110 13:38:09.754000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.758000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.760000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.762000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 50700 hash value: 15245130564338011730
[rank0]:W1110 13:38:09.763000 103726 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 141481570789279577
[rank0]:W1110 13:38:09.768000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank7]:W1110 13:38:09.768000 103733 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7248302464884289905
[rank1]:W1110 13:38:09.768000 103727 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 6861738078564757080
[rank3]:W1110 13:38:09.768000 103729 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 2920094510916107846
[rank2]:W1110 13:38:09.768000 103728 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 242796673245879148
[rank4]:W1110 13:38:09.768000 103730 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 15913116523503831995
[rank5]:W1110 13:38:09.768000 103731 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9924849963774050003
[rank6]:W1110 13:38:09.768000 103732 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5651206794127951651
[rank0]:W1110 13:38:09.769000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.773000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.774000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.775000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.776000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.777000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.778000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 17305501747464401909
[rank0]:W1110 13:38:09.779000 103726 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5199231758333309780
[rank0]:W1110 13:38:09.783000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.784000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.785000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.786000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.787000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.788000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.789000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
[rank0]:W1110 13:38:09.790000 103726 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 13980750043212511038
2025-11-10:13:38:17 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:13:38:17 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
[rank0]:[W1110 13:38:18.753548036 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
