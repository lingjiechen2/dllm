The following values were not passed to `accelerate launch` and had defaults used instead:
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:09:26:58 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:09:26:58 INFO     [__main__:450] Selected Tasks: ['gsm8k_cot']
2025-11-10:09:26:58 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:09:26:58 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:09:26:58 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:02<00:12,  2.43s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:06<00:13,  3.50s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:11<00:11,  3.95s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:15<00:08,  4.18s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:19<00:04,  4.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.82s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.83s/it]
2025-11-10:09:27:31 INFO     [evaluator:305] gsm8k_cot: Using gen_kwargs: {'do_sample': False, 'until': ['Q:', '</s>', '<|im_end|>']}
2025-11-10:09:27:31 WARNING  [evaluator:324] Overwriting default num_fewshot of gsm8k_cot from 8 to 8
2025-11-10:09:27:31 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:09:27:31 INFO     [api.task:434] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/20 [00:00<?, ?it/s] 45%|████▌     | 9/20 [00:00<00:00, 86.26it/s] 90%|█████████ | 18/20 [00:00<00:00, 88.01it/s]100%|██████████| 20/20 [00:00<00:00, 87.66it/s]
2025-11-10:09:27:32 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0755d87400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:09:27:34 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0755d87400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/20 [00:00<?, ? examples/s]Map: 100%|██████████| 20/20 [00:00<00:00, 538.17 examples/s]
Generating...:   0%|          | 0/20 [00:00<?, ?it/s]Generating...:   5%|▌         | 1/20 [00:26<08:27, 26.72s/it]Generating...:  10%|█         | 2/20 [00:51<07:40, 25.58s/it]Generating...:  15%|█▌        | 3/20 [01:17<07:17, 25.71s/it]Generating...:  20%|██        | 4/20 [01:42<06:45, 25.36s/it]Generating...:  25%|██▌       | 5/20 [02:08<06:25, 25.73s/it]Generating...:  30%|███       | 6/20 [02:34<06:00, 25.78s/it]Generating...:  35%|███▌      | 7/20 [03:00<05:35, 25.77s/it]Generating...:  40%|████      | 8/20 [03:26<05:09, 25.82s/it]Generating...:  45%|████▌     | 9/20 [03:52<04:45, 25.98s/it]Generating...:  50%|█████     | 10/20 [04:18<04:19, 25.95s/it]Generating...:  55%|█████▌    | 11/20 [04:44<03:53, 25.93s/it]Generating...:  60%|██████    | 12/20 [05:10<03:27, 25.92s/it]Generating...:  65%|██████▌   | 13/20 [05:36<03:01, 25.91s/it]Generating...:  70%|███████   | 14/20 [06:01<02:35, 25.91s/it]Generating...:  75%|███████▌  | 15/20 [06:27<02:09, 25.90s/it]Generating...:  80%|████████  | 16/20 [06:53<01:43, 25.98s/it]Generating...:  85%|████████▌ | 17/20 [07:19<01:17, 25.95s/it]Generating...:  90%|█████████ | 18/20 [07:45<00:51, 25.94s/it]Generating...:  95%|█████████▌| 19/20 [08:10<00:25, 25.60s/it]Generating...: 100%|██████████| 20/20 [08:36<00:00, 25.68s/it]Generating...: 100%|██████████| 20/20 [08:36<00:00, 25.82s/it]
2025-11-10:09:36:18 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:09:36:18 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: gsm8k_cot
