The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:08:15.892463640 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:08:15.969987882 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:08:15.511056574 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:08:15.512202824 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [__main__:450] Selected Tasks: ['minerva_math']
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:08:15.535923602 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:08:15.536376838 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:08:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:08:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:08:15.546173167 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:08:15.547845706 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:08:16 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.11it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:05,  1.01s/it]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.03it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.10s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.18s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.22s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.25s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]


Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]
[rank2]:[W1110 14:08:27.285273478 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 14:08:27.285500182 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 14:08:27.288860517 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 14:08:27.289106842 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 14:08:27.289206349 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 14:08:27.289315708 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 14:08:27.289433534 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 14:08:27.289527512 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:44 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:44 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 352.31it/s]
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:44 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:44 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:44 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:44 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 353.46it/s]
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:45 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 363.65it/s]
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:45 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 357.18it/s]
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:45 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 6...
2025-11-10:14:08:45 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
100%|██████████| 10/10 [00:00<00:00, 354.72it/s]2025-11-10:14:08:45 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}

2025-11-10:14:08:45 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
100%|██████████| 10/10 [00:00<00:00, 356.65it/s]
2025-11-10:14:08:45 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 358.60it/s]
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_algebra from 4 to 4
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_counting_and_prob: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_counting_and_prob from 4 to 4
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_geometry: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_geometry from 4 to 4
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_intermediate_algebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_intermediate_algebra from 4 to 4
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_num_theory: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_num_theory from 4 to 4
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_prealgebra: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_prealgebra from 4 to 4
2025-11-10:14:08:47 INFO     [evaluator:305] minerva_math_precalc: Using gen_kwargs: {'until': ['Problem:'], 'do_sample': False, 'temperature': 0.0}
2025-11-10:14:08:47 WARNING  [evaluator:324] Overwriting default num_fewshot of minerva_math_precalc from 4 to 4
2025-11-10:14:08:47 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_algebra on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 358.65it/s]
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 7...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 4...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 0...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 6...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 1...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 5...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 2...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_counting_and_prob on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 404.23it/s]100%|██████████| 10/10 [00:00<00:00, 409.08it/s]
100%|██████████| 10/10 [00:00<00:00, 406.85it/s]

100%|██████████| 10/10 [00:00<00:00, 393.02it/s]100%|██████████| 10/10 [00:00<00:00, 391.88it/s]100%|██████████| 10/10 [00:00<00:00, 396.76it/s]


100%|██████████| 10/10 [00:00<00:00, 376.81it/s]100%|██████████| 10/10 [00:00<00:00, 380.47it/s]

2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 4...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 5...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 0...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 6...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 1...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 7...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 2...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_geometry on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 410.41it/s]
100%|██████████| 10/10 [00:00<00:00, 406.91it/s]
100%|██████████| 10/10 [00:00<00:00, 396.09it/s]100%|██████████| 10/10 [00:00<00:00, 399.42it/s]100%|██████████| 10/10 [00:00<00:00, 395.97it/s]
100%|██████████| 10/10 [00:00<00:00, 399.23it/s]100%|██████████| 10/10 [00:00<00:00, 390.75it/s]

100%|██████████| 10/10 [00:00<00:00, 388.34it/s]


2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 4...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 3...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 2...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 6...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 0...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 5...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 1...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_intermediate_algebra on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 407.80it/s]100%|██████████| 10/10 [00:00<00:00, 412.00it/s]

100%|██████████| 10/10 [00:00<00:00, 401.56it/s]100%|██████████| 10/10 [00:00<00:00, 399.55it/s]100%|██████████| 10/10 [00:00<00:00, 396.65it/s]100%|██████████| 10/10 [00:00<00:00, 400.69it/s]
100%|██████████| 10/10 [00:00<00:00, 398.66it/s]


100%|██████████| 10/10 [00:00<00:00, 385.11it/s]

2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 4...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 6...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 5...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 7...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 0...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 3...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 1...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_num_theory on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 411.14it/s]
100%|██████████| 10/10 [00:00<00:00, 404.89it/s]100%|██████████| 10/10 [00:00<00:00, 408.52it/s]

100%|██████████| 10/10 [00:00<00:00, 398.41it/s]100%|██████████| 10/10 [00:00<00:00, 377.41it/s]100%|██████████| 10/10 [00:00<00:00, 392.22it/s]100%|██████████| 10/10 [00:00<00:00, 389.86it/s]

100%|██████████| 10/10 [00:00<00:00, 380.64it/s]


2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 4...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 6...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 3...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 5...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 0...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 1...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 2...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_prealgebra on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 412.14it/s]
100%|██████████| 10/10 [00:00<00:00, 406.90it/s]100%|██████████| 10/10 [00:00<00:00, 404.11it/s]
100%|██████████| 10/10 [00:00<00:00, 402.47it/s]
100%|██████████| 10/10 [00:00<00:00, 395.17it/s]100%|██████████| 10/10 [00:00<00:00, 400.15it/s]
100%|██████████| 10/10 [00:00<00:00, 384.70it/s]

100%|██████████| 10/10 [00:00<00:00, 375.13it/s]

2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 4...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 5...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 0...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 6...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 7...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 2...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 1...
2025-11-10:14:08:47 INFO     [api.task:434] Building contexts for minerva_math_precalc on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 409.88it/s]
100%|██████████| 10/10 [00:00<00:00, 405.21it/s]100%|██████████| 10/10 [00:00<00:00, 409.32it/s]

100%|██████████| 10/10 [00:00<00:00, 393.94it/s]100%|██████████| 10/10 [00:00<00:00, 398.79it/s]100%|██████████| 10/10 [00:00<00:00, 397.25it/s]100%|██████████| 10/10 [00:00<00:00, 401.32it/s]
100%|██████████| 10/10 [00:00<00:00, 394.33it/s]



2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:08:47 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f3ae4753400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f3ae4753400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fb03bcdf490> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fb03bcdf490> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f1cdc1a7400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f1cdc1a7400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f1eb40df400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f1eb40df400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map:   0%|          | 0/70 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f09a8587400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f09a8587400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f855c1670a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f855c1670a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2e20477400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2e20477400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f70b01f7400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]2025-11-10:14:08:50 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f70b01f7400> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/70 [00:00<?, ? examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 697.91 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 685.61 examples/s]
Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Map: 100%|██████████| 70/70 [00:00<00:00, 674.51 examples/s]
Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Map:  61%|██████▏   | 43/70 [00:00<00:00, 424.01 examples/s]Map:  63%|██████▎   | 44/70 [00:00<00:00, 424.00 examples/s]Map:  67%|██████▋   | 47/70 [00:00<00:00, 453.27 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 691.70 examples/s]Map: 100%|██████████| 70/70 [00:00<00:00, 681.14 examples/s]
Map: 100%|██████████| 70/70 [00:00<00:00, 668.88 examples/s]Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Map: 100%|██████████| 70/70 [00:00<00:00, 659.12 examples/s]
Map:  64%|██████▍   | 45/70 [00:00<00:00, 431.04 examples/s]Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Map: 100%|██████████| 70/70 [00:00<00:00, 477.42 examples/s]
Map: 100%|██████████| 70/70 [00:00<00:00, 472.04 examples/s]Generating...:   0%|          | 0/70 [00:00<?, ?it/s]
Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Map: 100%|██████████| 70/70 [00:00<00:00, 397.81 examples/s]
Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Map: 100%|██████████| 70/70 [00:00<00:00, 397.61 examples/s]
Generating...:   0%|          | 0/70 [00:00<?, ?it/s]Generating...:   1%|▏         | 1/70 [02:57<3:24:41, 177.99s/it]Generating...:   1%|▏         | 1/70 [02:58<3:24:43, 178.03s/it]Generating...:   1%|▏         | 1/70 [02:57<3:24:39, 177.96s/it]Generating...:   1%|▏         | 1/70 [02:58<3:24:45, 178.04s/it]Generating...:   1%|▏         | 1/70 [02:57<3:24:38, 177.95s/it]Generating...:   1%|▏         | 1/70 [02:58<3:24:44, 178.04s/it]Generating...:   1%|▏         | 1/70 [02:58<3:24:43, 178.03s/it]Generating...:   1%|▏         | 1/70 [02:57<3:24:41, 177.99s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:26, 184.80s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:25, 184.79s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:24, 184.78s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:23, 184.76s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:25, 184.79s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:23, 184.76s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:26, 184.80s/it]Generating...:   3%|▎         | 2/70 [06:07<3:29:24, 184.78s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:51, 181.66s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:52, 181.68s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:52, 181.68s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:52, 181.68s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:51, 181.67s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:51, 181.66s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:52, 181.68s/it]Generating...:   4%|▍         | 3/70 [09:05<3:22:51, 181.67s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.80s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.80s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.80s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.79s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.79s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.79s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:46, 179.80s/it]Generating...:   6%|▌         | 4/70 [12:02<3:17:45, 179.79s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:35, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:34, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:34, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:34, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:35, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:34, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:34, 178.69s/it]Generating...:   7%|▋         | 5/70 [14:59<3:13:34, 178.69s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:   9%|▊         | 6/70 [17:56<3:10:13, 178.34s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:05, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:05, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:05, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:05, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:05, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:05, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:04, 186.75s/it]Generating...:  10%|█         | 7/70 [21:20<3:16:04, 186.75s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  11%|█▏        | 8/70 [24:17<3:09:35, 183.48s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  13%|█▎        | 9/70 [27:31<3:09:58, 186.86s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  14%|█▍        | 10/70 [30:28<3:03:49, 183.82s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  16%|█▌        | 11/70 [33:32<3:00:41, 183.75s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  17%|█▋        | 12/70 [36:30<2:55:52, 181.95s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  19%|█▊        | 13/70 [39:27<2:51:33, 180.58s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  20%|██        | 14/70 [42:27<2:48:25, 180.46s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  21%|██▏       | 15/70 [45:25<2:44:35, 179.55s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  23%|██▎       | 16/70 [48:38<2:45:17, 183.66s/it]Generating...:  24%|██▍       | 17/70 [51:48<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:47<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:48<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:47<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:48<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:47<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:48<2:43:49, 185.46s/it]Generating...:  24%|██▍       | 17/70 [51:47<2:43:49, 185.46s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  26%|██▌       | 18/70 [54:48<2:39:27, 183.99s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  27%|██▋       | 19/70 [58:12<2:41:34, 190.08s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  29%|██▊       | 20/70 [1:01:26<2:39:13, 191.06s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  30%|███       | 21/70 [1:04:49<2:39:02, 194.74s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  31%|███▏      | 22/70 [1:08:24<2:40:36, 200.77s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  33%|███▎      | 23/70 [1:11:51<2:38:43, 202.62s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  34%|███▍      | 24/70 [1:15:07<2:33:52, 200.70s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  36%|███▌      | 25/70 [1:18:18<2:28:23, 197.85s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  37%|███▋      | 26/70 [1:21:32<2:24:11, 196.62s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  39%|███▊      | 27/70 [1:24:37<2:18:20, 193.03s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  40%|████      | 28/70 [1:27:54<2:16:07, 194.46s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:44, 194.24s/it]Generating...:  41%|████▏     | 29/70 [1:31:08<2:12:43, 194.24s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  43%|████▎     | 30/70 [1:34:22<2:09:28, 194.20s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  44%|████▍     | 31/70 [1:37:25<2:03:56, 190.69s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:26, 187.03s/it]Generating...:  46%|████▌     | 32/70 [1:40:23<1:58:27, 187.03s/it]Generating...:  47%|████▋     | 33/70 [1:44:43<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:43<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:43<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:43<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:42<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:43<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:42<2:08:42, 208.71s/it]Generating...:  47%|████▋     | 33/70 [1:44:43<2:08:42, 208.71s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  49%|████▊     | 34/70 [1:47:45<2:00:27, 200.77s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  50%|█████     | 35/70 [1:50:43<1:53:06, 193.91s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  51%|█████▏    | 36/70 [1:53:40<1:47:08, 189.08s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  53%|█████▎    | 37/70 [1:56:45<1:43:11, 187.61s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  54%|█████▍    | 38/70 [1:59:44<1:38:48, 185.27s/it]Generating...:  56%|█████▌    | 39/70 [2:02:45<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:45<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:44<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:44<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:44<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:44<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:44<1:34:54, 183.70s/it]Generating...:  56%|█████▌    | 39/70 [2:02:45<1:34:54, 183.70s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  57%|█████▋    | 40/70 [2:05:42<1:30:55, 181.83s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  59%|█████▊    | 41/70 [2:08:39<1:27:13, 180.47s/it]Generating...:  60%|██████    | 42/70 [2:11:38<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:39<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:39<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:38<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:39<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:39<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:39<1:24:03, 180.11s/it]Generating...:  60%|██████    | 42/70 [2:11:39<1:24:03, 180.11s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  61%|██████▏   | 43/70 [2:14:35<1:20:36, 179.12s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  63%|██████▎   | 44/70 [2:17:34<1:17:33, 178.96s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  64%|██████▍   | 45/70 [2:20:31<1:14:21, 178.46s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  66%|██████▌   | 46/70 [2:23:33<1:11:47, 179.48s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  67%|██████▋   | 47/70 [2:26:32<1:08:43, 179.28s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  69%|██████▊   | 48/70 [2:29:31<1:05:45, 179.32s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  70%|███████   | 49/70 [2:32:30<1:02:40, 179.08s/it]Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  71%|███████▏  | 50/70 [2:35:27<59:29, 178.49s/it]  Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  73%|███████▎  | 51/70 [2:38:33<57:14, 180.79s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  74%|███████▍  | 52/70 [2:41:38<54:35, 182.00s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  76%|███████▌  | 53/70 [2:45:26<55:31, 195.94s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  77%|███████▋  | 54/70 [2:48:42<52:12, 195.81s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  79%|███████▊  | 55/70 [2:51:38<47:27, 189.80s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  80%|████████  | 56/70 [2:54:40<43:47, 187.66s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  81%|████████▏ | 57/70 [2:57:40<40:06, 185.13s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  83%|████████▎ | 58/70 [3:00:38<36:38, 183.18s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  84%|████████▍ | 59/70 [3:03:42<33:37, 183.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  86%|████████▌ | 60/70 [3:06:42<30:24, 182.41s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  87%|████████▋ | 61/70 [3:09:57<27:55, 186.11s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  89%|████████▊ | 62/70 [3:12:56<24:32, 184.12s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  90%|█████████ | 63/70 [3:16:38<22:47, 195.36s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  91%|█████████▏| 64/70 [3:19:38<19:03, 190.67s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  93%|█████████▎| 65/70 [3:22:37<15:36, 187.37s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  94%|█████████▍| 66/70 [3:25:36<12:18, 184.72s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  96%|█████████▌| 67/70 [3:28:41<09:14, 184.93s/it]Generating...:  97%|█████████▋| 68/70 [3:31:54<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:55<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:54<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:54<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:55<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:54<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:54<06:14, 187.37s/it]Generating...:  97%|█████████▋| 68/70 [3:31:54<06:14, 187.37s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...:  99%|█████████▊| 69/70 [3:34:54<03:04, 184.97s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 185.24s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]
Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]Generating...: 100%|██████████| 70/70 [3:38:00<00:00, 186.86s/it]






[rank2]:W1110 17:46:54.672000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44672 hash value: 5209784832455863057
[rank6]:W1110 17:46:54.680000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44186 hash value: 14735786638852297516
[rank1]:W1110 17:46:54.704000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43147 hash value: 11442508508554870129
[rank5]:W1110 17:46:54.706000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 39723 hash value: 1035113776640566156
[rank4]:W1110 17:46:54.723000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44613 hash value: 4881825480546449401
[rank0]:W1110 17:46:54.805000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 38026 hash value: 13001281202974627921
[rank7]:W1110 17:46:54.934000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40666 hash value: 8038083666574280381
[rank3]:W1110 17:46:54.965000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40296 hash value: 461901666871273597
[rank0]:W1110 17:46:55.166000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.170000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.172000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.175000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.177000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.179000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.180000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank6]:W1110 17:46:55.181000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12760118203891625443
[rank1]:W1110 17:46:55.181000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.181000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 6475571178142554729
[rank5]:W1110 17:46:55.181000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 7249169887501832008
[rank3]:W1110 17:46:55.181000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 15165775158567111781
[rank4]:W1110 17:46:55.181000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 7249169887501832008
[rank7]:W1110 17:46:55.181000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415655568741
[rank0]:W1110 17:46:55.182000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 44672 hash value: 6430179176230729846
[rank0]:W1110 17:46:55.186000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 100762994949750259
[rank0]:W1110 17:46:55.190000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank1]:W1110 17:46:55.190000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 5477670933006870797
[rank3]:W1110 17:46:55.190000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 15208346818637871712
[rank5]:W1110 17:46:55.190000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 7249170280256129000
[rank2]:W1110 17:46:55.190000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 2903506003168758734
[rank7]:W1110 17:46:55.190000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9127604107462155897
[rank6]:W1110 17:46:55.190000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 1914875617057340183
[rank4]:W1110 17:46:55.190000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 10687580460066797789
[rank0]:W1110 17:46:55.192000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.195000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.196000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.197000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.198000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.199000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.200000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17888308262913448956
[rank0]:W1110 17:46:55.201000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 5098524144857893876
[rank0]:W1110 17:46:55.205000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank5]:W1110 17:46:55.205000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 55497 hash value: 10965903686596186884
[rank4]:W1110 17:46:55.205000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 43836 hash value: 12323012609136341257
[rank6]:W1110 17:46:55.205000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50726 hash value: 3193045102487308849
[rank1]:W1110 17:46:55.205000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 52562 hash value: 1054039490312285593
[rank2]:W1110 17:46:55.205000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49812 hash value: 5796617477432440766
[rank3]:W1110 17:46:55.206000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50597 hash value: 2757138997708375660
[rank7]:W1110 17:46:55.206000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50079 hash value: 13888039880707275102
[rank0]:W1110 17:46:55.206000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.210000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.211000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.212000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.213000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.214000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.215000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 16383813000571995847
[rank0]:W1110 17:46:55.216000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 51363 hash value: 8564704072348757718
[rank6]:W1110 17:46:55.220000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.220000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 7824157749669262784
[rank5]:W1110 17:46:55.220000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank1]:W1110 17:46:55.220000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 8163468435338816897
[rank7]:W1110 17:46:55.220000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank3]:W1110 17:46:55.220000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank4]:W1110 17:46:55.220000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 2080527501599604670
[rank0]:W1110 17:46:55.221000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.226000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.228000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.230000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.232000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.234000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.236000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.238000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55497 hash value: 10430209132194041571
[rank0]:W1110 17:46:55.239000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 100762994949746626
[rank0]:W1110 17:46:55.243000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank1]:W1110 17:46:55.243000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 8163468435338816897
[rank6]:W1110 17:46:55.243000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 10978510432726235580
[rank5]:W1110 17:46:55.243000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12760118207575638993
[rank7]:W1110 17:46:55.243000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580209614566759
[rank4]:W1110 17:46:55.243000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 10978510516341575934
[rank3]:W1110 17:46:55.243000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 6481603594869254847
[rank2]:W1110 17:46:55.243000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 16860987534581191490
[rank0]:W1110 17:46:55.245000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.248000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.249000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.250000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.251000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.252000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.253000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6122624308291595872
[rank0]:W1110 17:46:55.254000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12325176233443528632
[rank0]:W1110 17:46:55.258000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank1]:W1110 17:46:55.258000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 55510 hash value: 11035418813366153942
[rank2]:W1110 17:46:55.258000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 58612 hash value: 9827410972454649424
[rank5]:W1110 17:46:55.258000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 57604 hash value: 5491247128298166723
[rank6]:W1110 17:46:55.258000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 55731 hash value: 11193434055182188405
[rank4]:W1110 17:46:55.258000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 59361 hash value: 14190617183290198419
[rank3]:W1110 17:46:55.258000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 58983 hash value: 8843382319611567071
[rank7]:W1110 17:46:55.258000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 62454 hash value: 14043590326994577292
[rank0]:W1110 17:46:55.259000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.262000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.263000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.264000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.265000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.266000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.267000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 4446297754338656702
[rank0]:W1110 17:46:55.268000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 59949 hash value: 17826149461882941119
[rank6]:W1110 17:46:55.273000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908223210
[rank5]:W1110 17:46:55.273000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 6475571178142554729
[rank7]:W1110 17:46:55.273000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank1]:W1110 17:46:55.273000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank4]:W1110 17:46:55.273000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank3]:W1110 17:46:55.273000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.273000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank0]:W1110 17:46:55.274000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.279000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.281000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.283000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.286000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.288000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.290000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.292000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 62454 hash value: 4501173451054613640
[rank0]:W1110 17:46:55.293000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank0]:W1110 17:46:55.297000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank6]:W1110 17:46:55.297000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 16860987475405446805
[rank5]:W1110 17:46:55.297000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 13800512212344007879
[rank7]:W1110 17:46:55.297000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12760118203891625443
[rank4]:W1110 17:46:55.297000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 13855304786694522264
[rank2]:W1110 17:46:55.297000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12760118203891809526
[rank1]:W1110 17:46:55.297000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 8045969157778582136
[rank3]:W1110 17:46:55.297000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12826790799051801796
[rank0]:W1110 17:46:55.299000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.302000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.303000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.304000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.305000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.306000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.307000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10773337577934520465
[rank0]:W1110 17:46:55.308000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12826790799051813943
[rank0]:W1110 17:46:55.312000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank6]:W1110 17:46:55.313000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 53043 hash value: 7443553866256406637
[rank4]:W1110 17:46:55.313000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 52771 hash value: 15738011837953164454
[rank5]:W1110 17:46:55.313000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 54237 hash value: 2348723340818217929
[rank7]:W1110 17:46:55.313000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 55294 hash value: 15307701371095438846
[rank1]:W1110 17:46:55.313000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 54328 hash value: 7747651308251036463
[rank2]:W1110 17:46:55.313000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 55206 hash value: 4538475851838911233
[rank3]:W1110 17:46:55.313000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 49504 hash value: 15793202905524820242
[rank0]:W1110 17:46:55.314000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.317000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.318000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.319000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.320000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.321000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.322000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9041807593732469162
[rank0]:W1110 17:46:55.323000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 53926 hash value: 3076112538492326151
[rank6]:W1110 17:46:55.327000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank7]:W1110 17:46:55.327000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank5]:W1110 17:46:55.327000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank4]:W1110 17:46:55.327000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.327000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank1]:W1110 17:46:55.327000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank3]:W1110 17:46:55.327000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908223210
[rank0]:W1110 17:46:55.328000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.333000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.335000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.337000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.339000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.341000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.343000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.345000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 55294 hash value: 5130919162133753938
[rank0]:W1110 17:46:55.346000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank0]:W1110 17:46:55.350000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank5]:W1110 17:46:55.350000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415655568741
[rank6]:W1110 17:46:55.350000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.350000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank7]:W1110 17:46:55.350000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908223210
[rank3]:W1110 17:46:55.350000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908223210
[rank1]:W1110 17:46:55.350000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 7391278077296346813
[rank4]:W1110 17:46:55.350000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 2080530102404890844
[rank0]:W1110 17:46:55.352000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.355000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.356000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.357000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.358000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.359000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.360000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 10191613041285025728
[rank0]:W1110 17:46:55.361000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 2080530102404780745
[rank0]:W1110 17:46:55.365000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank1]:W1110 17:46:55.365000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 46054 hash value: 5580463375695057702
[rank6]:W1110 17:46:55.365000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50820 hash value: 6068549851769215940
[rank5]:W1110 17:46:55.365000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50102 hash value: 11696652399841832007
[rank2]:W1110 17:46:55.365000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 53789 hash value: 14397803382991946294
[rank7]:W1110 17:46:55.365000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 53203 hash value: 5275324019829730649
[rank3]:W1110 17:46:55.365000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 48334 hash value: 8762311292482498856
[rank4]:W1110 17:46:55.365000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 50678 hash value: 6515514061241560584
[rank0]:W1110 17:46:55.366000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.370000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.371000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.372000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.373000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.374000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.375000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 8740806973604362
[rank0]:W1110 17:46:55.376000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 51500 hash value: 16279578687455418318
[rank5]:W1110 17:46:55.380000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 6475571178142554729
[rank6]:W1110 17:46:55.380000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank7]:W1110 17:46:55.380000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank1]:W1110 17:46:55.380000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12759702263565105956
[rank3]:W1110 17:46:55.380000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9223449677976106806
[rank4]:W1110 17:46:55.380000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.380000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank0]:W1110 17:46:55.381000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.387000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.389000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.391000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.393000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.395000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.397000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.399000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 53789 hash value: 13067585184586489282
[rank0]:W1110 17:46:55.400000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9223449677976106806
[rank0]:W1110 17:46:55.404000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank3]:W1110 17:46:55.404000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 5232175015349135601
[rank5]:W1110 17:46:55.404000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 6462084732983894855
[rank6]:W1110 17:46:55.404000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12760118203891625443
[rank7]:W1110 17:46:55.404000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 13855304788125529212
[rank4]:W1110 17:46:55.404000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12759702263565105956
[rank1]:W1110 17:46:55.404000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 5135111188951365318
[rank2]:W1110 17:46:55.404000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 7391363343754013323
[rank0]:W1110 17:46:55.406000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.409000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.410000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.411000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.412000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.413000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.414000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11041187369435350846
[rank0]:W1110 17:46:55.415000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9707684007058040015
[rank0]:W1110 17:46:55.419000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank1]:W1110 17:46:55.419000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 41976 hash value: 7278416990970956080
[rank6]:W1110 17:46:55.419000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 41923 hash value: 9794073776782940377
[rank5]:W1110 17:46:55.419000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 42720 hash value: 3370053912338703698
[rank4]:W1110 17:46:55.419000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 39679 hash value: 9422422588845219258
[rank2]:W1110 17:46:55.419000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37775 hash value: 12156821617853387172
[rank3]:W1110 17:46:55.419000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 45302 hash value: 10034977118990203173
[rank7]:W1110 17:46:55.419000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37926 hash value: 8072089676344593575
[rank0]:W1110 17:46:55.420000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.424000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.425000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.426000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.427000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.428000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.429000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 9738214428142212766
[rank0]:W1110 17:46:55.430000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 44029 hash value: 11782618204314762131
[rank6]:W1110 17:46:55.434000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank7]:W1110 17:46:55.434000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580209005957337
[rank2]:W1110 17:46:55.434000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9223449677976076351
[rank1]:W1110 17:46:55.434000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 16858811488333850520
[rank5]:W1110 17:46:55.434000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 2511766163463039167
[rank3]:W1110 17:46:55.434000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 6475571178142554729
[rank4]:W1110 17:46:55.434000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9799981658433655582
[rank0]:W1110 17:46:55.435000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.439000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.441000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.443000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.445000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.447000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.448000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.450000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 45302 hash value: 7301477557156718421
[rank0]:W1110 17:46:55.451000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580209614566759
[rank0]:W1110 17:46:55.455000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank7]:W1110 17:46:55.456000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 13116726536427929440
[rank6]:W1110 17:46:55.456000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 16860987475405446805
[rank1]:W1110 17:46:55.456000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 8891434816367507454
[rank5]:W1110 17:46:55.456000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 16444192185569268560
[rank2]:W1110 17:46:55.456000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 13293026212700898831
[rank4]:W1110 17:46:55.456000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 10690696237565283102
[rank3]:W1110 17:46:55.456000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 9799981658928388624
[rank0]:W1110 17:46:55.458000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.461000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.462000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.463000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.463000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.464000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.465000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 11640111062801483380
[rank0]:W1110 17:46:55.466000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 17937861279654764356
[rank0]:W1110 17:46:55.470000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank6]:W1110 17:46:55.471000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 51022 hash value: 877948905561187803
[rank5]:W1110 17:46:55.471000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 58091 hash value: 6251726541400979577
[rank3]:W1110 17:46:55.471000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 57723 hash value: 5669142328716413880
[rank4]:W1110 17:46:55.471000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 61092 hash value: 13888662881847038224
[rank2]:W1110 17:46:55.471000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 53720 hash value: 1612503350102680745
[rank7]:W1110 17:46:55.471000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 59670 hash value: 5201764876236085500
[rank1]:W1110 17:46:55.471000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 63584 hash value: 4937722570726010725
[rank0]:W1110 17:46:55.472000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.475000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.476000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.477000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.478000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.479000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.480000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 6525810178387989528
[rank0]:W1110 17:46:55.481000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 60301 hash value: 9695758446603339900
[rank7]:W1110 17:46:55.485000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank4]:W1110 17:46:55.485000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank5]:W1110 17:46:55.485000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank6]:W1110 17:46:55.485000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank2]:W1110 17:46:55.485000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415655568741
[rank1]:W1110 17:46:55.485000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank3]:W1110 17:46:55.485000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank0]:W1110 17:46:55.486000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.491000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.494000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.496000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.498000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.500000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.502000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.505000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 63584 hash value: 9749437651637747746
[rank0]:W1110 17:46:55.506000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank0]:W1110 17:46:55.510000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank5]:W1110 17:46:55.510000 81649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908222809
[rank6]:W1110 17:46:55.510000 81650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 15165774989672468480
[rank3]:W1110 17:46:55.510000 81646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 100762994949746626
[rank2]:W1110 17:46:55.510000 81645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 8422895373505614218
[rank4]:W1110 17:46:55.510000 81648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415908223210
[rank7]:W1110 17:46:55.510000 81651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12759981406545507419
[rank1]:W1110 17:46:55.510000 81644 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 14941580415655568741
[rank0]:W1110 17:46:55.512000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.515000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.517000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.518000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.518000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.519000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.520000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 17378403494878580535
[rank0]:W1110 17:46:55.521000 81643 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36 hash value: 12325180052725017288
[rank0]:W1110 17:46:55.525000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.526000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.527000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.528000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.529000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.530000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.531000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
[rank0]:W1110 17:46:55.532000 81643 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36 hash value: 15439332785123291615
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_algebra
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_counting_and_prob
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_geometry
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_intermediate_algebra
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_num_theory
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_prealgebra
2025-11-10:17:47:03 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: minerva_math_precalc
[rank0]:[W1110 17:47:04.608710015 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
