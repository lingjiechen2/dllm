The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1113 03:18:15.282901918 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:18:15.282903892 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:18:15.282905004 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-13:03:18:15 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1113 03:18:15.554360057 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:18:15.554796702 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-13:03:18:15 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-13:03:18:15 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 1024, 'steps': 1024, 'block_length': 1024, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1113 03:18:15.560225581 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:18:15.561251379 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1113 03:18:15.564214374 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-13:03:18:15 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.14it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.16it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:04,  1.01s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:04,  1.01s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.06s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.06s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.10s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.10s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.20s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.19s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.08s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.14s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.17s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.15s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.08s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.13s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.17s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.18s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.02s/it]
Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.27s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.27s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]

Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]
[rank0]:[W1113 03:18:27.213934484 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1113 03:18:27.214008243 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1113 03:18:27.214252160 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1113 03:18:27.214283636 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1113 03:18:27.214310461 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1113 03:18:27.214500501 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1113 03:18:27.214721809 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1113 03:18:27.214776156 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-13:03:18:37 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:37 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:37 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:37 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 842.59it/s]
2025-11-13:03:18:37 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:37 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:37 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:37 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 846.96it/s]
2025-11-13:03:18:37 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:37 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:37 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:37 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 845.88it/s]
2025-11-13:03:18:38 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:38 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:38 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:38 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 845.80it/s]
2025-11-13:03:18:38 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:38 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:38 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:38 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 848.23it/s]
2025-11-13:03:18:38 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:38 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:38 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:38 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 835.82it/s]
2025-11-13:03:18:38 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:38 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:38 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:38 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 838.81it/s]
2025-11-13:03:18:38 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-13:03:18:38 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-13:03:18:38 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-13:03:18:38 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 848.50it/s]
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
2025-11-13:03:18:39 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa49411a440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa49411a440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5cec1663b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5cec1663b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f9bbc146440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f9bbc146440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f34dd28e440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f34dd28e440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7efb1c1fa440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7efb1c1fa440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fbe42072440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fbe42072440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|██████████| 10/10 [00:00<00:00, 764.53 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 641.36 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 876.31 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f67cffde3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f3a15e1e440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f67cffde3b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-13:03:18:41 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f3a15e1e440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|██████████| 10/10 [00:00<00:00, 782.55 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 852.24 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 783.48 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 856.23 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 723.21 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.47s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.48s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.46s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.48s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.47s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.47s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.46s/it]Generating...:  10%|█         | 1/10 [02:11<19:43, 131.48s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.20s/it]Generating...:  20%|██        | 2/10 [04:30<18:09, 136.19s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.72s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.73s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.73s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.73s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.73s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.73s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.72s/it]Generating...:  30%|███       | 3/10 [06:52<16:11, 138.73s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  40%|████      | 4/10 [09:02<13:30, 135.02s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  50%|█████     | 5/10 [11:34<11:46, 141.21s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  60%|██████    | 6/10 [13:53<09:22, 140.65s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  70%|███████   | 7/10 [16:05<06:53, 137.67s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  80%|████████  | 8/10 [18:18<04:32, 136.21s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.18s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.19s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.19s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.19s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.18s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.19s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.19s/it]Generating...:  90%|█████████ | 9/10 [21:16<02:29, 149.19s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 156.66s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]
Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]

Generating...: 100%|██████████| 10/10 [24:09<00:00, 144.95s/it]




[rank2]:W1113 03:42:56.084000 45647 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 34433 hash value: 11599163167111788420
[rank4]:W1113 03:42:57.200000 45649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 42174 hash value: 2513837292474328517
[rank6]:W1113 03:42:59.658000 45651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40529 hash value: 3884756332146360410
[rank7]:W1113 03:42:59.863000 45652 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37893 hash value: 2274409662826903120
[rank5]:W1113 03:43:00.533000 45650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 41512 hash value: 13856147733537024152
[rank3]:W1113 03:43:00.784000 45648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40158 hash value: 15240087177115683648
[rank1]:W1113 03:43:01.220000 45646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40385 hash value: 5204844989789853236
[rank0]:W1113 03:43:01.879000 45645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 47442 hash value: 11081330107098173230
[rank0]:W1113 03:43:02.069000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.071000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.074000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.076000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.078000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank6]:W1113 03:43:02.079000 45651 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 10462387475064696176
[rank1]:W1113 03:43:02.079000 45646 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11888772715519004810
[rank5]:W1113 03:43:02.079000 45650 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9686548071315526909
[rank2]:W1113 03:43:02.079000 45647 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5032981802388863815
[rank7]:W1113 03:43:02.079000 45652 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 7986107118602456524
[rank4]:W1113 03:43:02.079000 45649 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14712373174771148005
[rank3]:W1113 03:43:02.079000 45648 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1892600311987657058
[rank0]:W1113 03:43:02.080000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.084000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.086000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 47442 hash value: 14375105916140447233
[rank0]:W1113 03:43:02.087000 45645 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 6102112901267973909
[rank0]:W1113 03:43:02.092000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.093000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.094000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.094000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.095000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.096000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.097000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
[rank0]:W1113 03:43:02.098000 45645 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10169073017162043085
2025-11-13:03:43:09 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-13:03:43:09 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
[rank0]:[W1113 03:43:10.052412137 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
