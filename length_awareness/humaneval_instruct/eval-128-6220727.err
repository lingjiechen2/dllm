The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:14:11:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:16 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:16 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:16 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:16 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:11:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:11:16 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:16 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:11:16.501908913 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:11:16.501908754 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:11:16.501910554 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:11:17 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:17 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:17 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:17 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:17 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:17 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:11:17 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:17 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:11:17.847234313 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:11:17.849127702 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:11:17 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:17 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:17 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:17 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:17 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:17 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:11:17 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:17 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:11:17.870079797 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:11:17.870828959 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:11:17 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:11:17 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:11:17 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:11:17 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 128, 'steps': 128, 'block_length': 128, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:11:17.947676555 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:11:17 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.05it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.09it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.04it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.14s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.14s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.29s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.30s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.29s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.30s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.31s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.30s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.30s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.30s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.30s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.30s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.30s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.30s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.30s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]




Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]
[rank5]:[W1110 14:11:29.866742960 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 14:11:29.866885548 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 14:11:29.867404545 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 14:11:29.868091213 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 14:11:29.868115824 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 14:11:29.868529745 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 14:11:29.868661732 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 14:11:29.869040365 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:11:40 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:40 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:40 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:40 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:40 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 2...
2025-11-10:14:11:40 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 832.76it/s]
100%|██████████| 10/10 [00:00<00:00, 827.12it/s]
2025-11-10:14:11:40 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:40 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:40 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 829.87it/s]
2025-11-10:14:11:40 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:40 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:40 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 832.72it/s]
2025-11-10:14:11:40 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:40 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:40 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:40 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 834.60it/s]
2025-11-10:14:11:41 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:41 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:41 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:41 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 829.55it/s]
2025-11-10:14:11:42 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:42 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:42 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:42 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 833.36it/s]
2025-11-10:14:11:42 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:11:42 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:11:42 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:11:42 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 837.72it/s]
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:11:42 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc108172560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fc108172560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0a09e12560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0a09e12560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f48e80f6560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f48e80f6560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f61823ba4d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f61823ba4d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f9509f0e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f9509f0e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 836.37 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f679c10a560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|██████████| 10/10 [00:00<00:00, 736.95 examples/s]2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f679c10a560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f16bc1a6560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:14:11:44 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f16bc1a6560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 801.05 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 825.20 examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa3c019a560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Generating...:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:11:45 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fa3c019a560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 837.65 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 949.45 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 870.03 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 853.19 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.26s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.25s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.26s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.25s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.24s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.25s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.24s/it]Generating...:  10%|█         | 1/10 [00:07<01:05,  7.25s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  20%|██        | 2/10 [00:14<00:59,  7.40s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.54s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.54s/it]Generating...:  30%|███       | 3/10 [00:22<00:52,  7.55s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  40%|████      | 4/10 [00:28<00:42,  7.01s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  50%|█████     | 5/10 [00:37<00:38,  7.69s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  60%|██████    | 6/10 [00:45<00:30,  7.67s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  70%|███████   | 7/10 [00:51<00:21,  7.23s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  80%|████████  | 8/10 [00:58<00:14,  7.02s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...:  90%|█████████ | 9/10 [01:09<00:08,  8.51s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  9.39s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]Generating...: 100%|██████████| 10/10 [01:21<00:00,  8.12s/it]







[rank6]:W1110 14:13:08.720000 37039 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37588 hash value: 11882063731094191930
[rank5]:W1110 14:13:12.764000 37038 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 35699 hash value: 3280689536145527869
[rank0]:W1110 14:13:13.072000 37032 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 38863 hash value: 14148835099076755259
[rank4]:W1110 14:13:13.961000 37037 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 38480 hash value: 8330355197723568215
[rank3]:W1110 14:13:14.909000 37035 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 34961 hash value: 17208962309370605574
[rank2]:W1110 14:13:15.632000 37034 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 34351 hash value: 3731849115613948332
[rank1]:W1110 14:13:16.418000 37033 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 39423 hash value: 2145211930743544953
[rank7]:W1110 14:13:17.013000 37040 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 32289 hash value: 13387209734405027115
[rank0]:W1110 14:13:17.201000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.203000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.206000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank6]:W1110 14:13:17.206000 37039 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 16779918209445092380
[rank4]:W1110 14:13:17.206000 37037 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14712373174771148005
[rank1]:W1110 14:13:17.206000 37033 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 11090555702699821886
[rank2]:W1110 14:13:17.206000 37034 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5032981802388863815
[rank5]:W1110 14:13:17.206000 37038 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 18105060839511562746
[rank7]:W1110 14:13:17.206000 37040 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9601867242340079822
[rank3]:W1110 14:13:17.206000 37035 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 8845860186250493752
[rank0]:W1110 14:13:17.207000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.212000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.214000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.216000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.218000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 39423 hash value: 11366245452852610659
[rank0]:W1110 14:13:17.219000 37032 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13926372034106151649
[rank0]:W1110 14:13:17.223000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.224000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.225000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.227000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.228000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.229000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.230000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
[rank0]:W1110 14:13:17.231000 37032 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 9569087537416197064
2025-11-10:14:13:24 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:14:13:24 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
[rank0]:[W1110 14:13:25.249847339 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
