The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:14:12:28 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:28 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:28 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:28 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:28 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:28 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:28 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:28 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:28 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:12:28 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:28.528369228 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:12:28.528383719 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:28 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:28 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:28.535788728 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:28 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:28 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:28 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:28 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:28 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:28 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:28 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:28 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:12:28 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:28 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:28.605212868 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:28 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
[W1110 14:12:28.608950695 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:28 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:28.613798119 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:29 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:29 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:29 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:29 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:29.726101497 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:29 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:29 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:29 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:29 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 512, 'steps': 512, 'block_length': 512, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:29.849860246 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:29 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.17it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.14it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.14it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.14it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.08it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:04,  1.01s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:04,  1.01s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:01<00:03,  1.00it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.05s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.09s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.13s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.11s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.15s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.15s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.15s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.19s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.23s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.10s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.15s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.16s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.15s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.20s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.24s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.07s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.15s/it]
[rank4]:[W1110 14:12:40.391650056 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 14:12:40.391869401 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 14:12:40.391914795 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 14:12:40.394453222 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 14:12:40.396301004 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 14:12:40.396345800 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 14:12:40.396515918 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 14:12:40.396597726 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:51 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:51 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:51 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:51 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 843.13it/s]
2025-11-10:14:12:51 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:51 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:51 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:51 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 832.24it/s]
2025-11-10:14:12:51 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:51 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:51 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:51 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 831.23it/s]
2025-11-10:14:12:52 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:52 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:52 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:52 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:12:52 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:52 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:52 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:52 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 832.30it/s]
100%|██████████| 10/10 [00:00<00:00, 841.27it/s]
2025-11-10:14:12:52 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:52 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:52 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:52 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 838.26it/s]
2025-11-10:14:12:52 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:52 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:52 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:52 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 845.56it/s]
2025-11-10:14:12:52 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:52 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:52 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:52 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 846.21it/s]
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:52 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fba18196560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fba18196560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f043418e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f043418e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5f39fba560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f5f39fba560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fef29792560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fef29792560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 861.66 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 735.92 examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f407c0b6560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Generating...:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f407c0b6560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f902411e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe9840fe560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f902411e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f6e10186560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fe9840fe560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:55 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f6e10186560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 857.87 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 800.35 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 837.20 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 912.90 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 762.07 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 766.42 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.37s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.39s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.39s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.38s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.39s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.38s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.37s/it]Generating...:  10%|█         | 1/10 [00:45<06:48, 45.37s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.51s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.52s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.51s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.51s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.51s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.52s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.51s/it]Generating...:  20%|██        | 2/10 [01:32<06:12, 46.51s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.10s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.09s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.09s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.09s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.10s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.10s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.10s/it]Generating...:  30%|███       | 3/10 [02:20<05:29, 47.09s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  40%|████      | 4/10 [03:05<04:38, 46.35s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  50%|█████     | 5/10 [03:59<04:04, 48.87s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  60%|██████    | 6/10 [04:46<03:14, 48.54s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  70%|███████   | 7/10 [05:33<02:23, 47.83s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.43s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.42s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.42s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.42s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.42s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.42s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.43s/it]Generating...:  80%|████████  | 8/10 [06:19<01:34, 47.42s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...:  90%|█████████ | 9/10 [07:19<00:51, 51.23s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 53.60s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.83s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.83s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.84s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.83s/it]
Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.83s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.84s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.83s/it]Generating...: 100%|██████████| 10/10 [08:18<00:00, 49.83s/it]






[rank0]:W1110 14:21:19.001000 38141 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 42769 hash value: 1470708470710154022
[rank6]:W1110 14:21:19.745000 38148 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40042 hash value: 190926794968941442
[rank5]:W1110 14:21:19.956000 38147 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36225 hash value: 4372055898688961426
[rank1]:W1110 14:21:21.038000 38142 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 41710 hash value: 11655812500518682944
[rank2]:W1110 14:21:21.964000 38144 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 33746 hash value: 1156817061251079124
[rank7]:W1110 14:21:22.153000 38149 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 34474 hash value: 11534261885829205790
[rank4]:W1110 14:21:22.734000 38146 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 39902 hash value: 2231673331500214858
[rank3]:W1110 14:21:23.119000 38145 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37860 hash value: 15762033235948405153
[rank0]:W1110 14:21:23.302000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.305000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.308000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.309000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank3]:W1110 14:21:23.310000 38145 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 505179071798723510
[rank2]:W1110 14:21:23.310000 38144 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5032981802388863815
[rank4]:W1110 14:21:23.310000 38146 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14712373174771148005
[rank7]:W1110 14:21:23.310000 38149 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13926372034106151649
[rank1]:W1110 14:21:23.310000 38142 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12722746633449006015
[rank5]:W1110 14:21:23.310000 38147 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 9686548071315526909
[rank6]:W1110 14:21:23.311000 38148 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 10462387475064696176
[rank0]:W1110 14:21:23.311000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.316000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.318000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.319000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 42769 hash value: 3294112088595524717
[rank0]:W1110 14:21:23.321000 38141 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 3135330617490341091
[rank0]:W1110 14:21:23.325000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.326000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.328000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.329000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.329000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.330000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.331000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
[rank0]:W1110 14:21:23.332000 38141 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 11328317742702203042
2025-11-10:14:21:30 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:14:21:30 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
[rank0]:[W1110 14:21:31.215730321 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
