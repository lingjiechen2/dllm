The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 14:09:53.612883609 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:09:53.612886983 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:09:53.612893207 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
[W1110 14:09:53.812834644 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 14:09:53.827503720 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:09:53.829158807 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 14:09:53.905385675 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:09:53 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:09:53 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:09:53 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:09:53 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 64, 'steps': 64, 'block_length': 64, 'cfg': 0.0, 'confidence_eos_eot_inf':
        True}
[W1110 14:09:53.018241279 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:09:54 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.13it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.11it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.12it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.05it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.13it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.05it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.05s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.04s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.05s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.11s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.13s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.14s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.20s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.28s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.29s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.28s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.15s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.17s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:05<00:01,  1.20s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.31s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.33s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.33s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.33s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.09s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.08s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.22s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.23s/it]


[rank3]:[W1110 14:10:05.939821763 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 14:10:05.940103073 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 14:10:05.940137841 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank0]:[W1110 14:10:05.940531761 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 14:10:05.940564057 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 14:10:05.940721388 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 14:10:05.940753678 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 14:10:05.940995903 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:10:16 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:16 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:16 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:16 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 828.39it/s]
2025-11-10:14:10:17 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:17 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:17 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:17 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 832.83it/s]
2025-11-10:14:10:17 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:17 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:17 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:17 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 831.87it/s]
2025-11-10:14:10:17 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:17 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:17 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:17 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:10:17 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:17 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:17 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:17 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 813.59it/s]
100%|██████████| 10/10 [00:00<00:00, 841.64it/s]
2025-11-10:14:10:17 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:17 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:17 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:17 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 836.12it/s]
2025-11-10:14:10:18 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:18 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:18 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:18 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 828.03it/s]
2025-11-10:14:10:18 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:10:18 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:10:18 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:10:18 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 831.35it/s]
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:10:18 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f7714106710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f7714106710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f08e40de710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f08e40de710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f15ac0d6710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f15ac0d6710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2801f46710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2801f46710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 779.07 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 818.56 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0ad0de2710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f0ad0de2710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f8b5cb3e680> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f8b5cb3e680> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 971.51 examples/s]
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f7a6cd6a710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f7a6cd6a710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fd36419e710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:14:10:21 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fd36419e710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 833.79 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 829.95 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 740.73 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 829.90 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 884.97 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.49s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.50s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.50s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.50s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.48s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.48s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.49s/it]Generating...:  10%|█         | 1/10 [00:03<00:31,  3.48s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  20%|██        | 2/10 [00:06<00:26,  3.28s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.23s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.23s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.23s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.23s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.23s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.24s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.23s/it]Generating...:  30%|███       | 3/10 [00:09<00:22,  3.24s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  40%|████      | 4/10 [00:12<00:17,  2.93s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.41s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.40s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.40s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.40s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.40s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.40s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.41s/it]Generating...:  50%|█████     | 5/10 [00:16<00:17,  3.40s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  60%|██████    | 6/10 [00:19<00:13,  3.31s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  70%|███████   | 7/10 [00:22<00:09,  3.21s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  80%|████████  | 8/10 [00:25<00:06,  3.17s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...:  90%|█████████ | 9/10 [00:31<00:03,  3.91s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  4.09s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]Generating...: 100%|██████████| 10/10 [00:35<00:00,  3.58s/it]







[rank3]:W1110 14:11:00.598000 21479 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 33032 hash value: 3295438632110448604
[rank2]:W1110 14:11:03.421000 21477 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 31641 hash value: 11702117940161370083
[rank6]:W1110 14:11:05.137000 21482 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 35444 hash value: 9547707570553783472
[rank7]:W1110 14:11:05.751000 21485 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 29467 hash value: 11988042699042355145
[rank4]:W1110 14:11:06.135000 21480 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 35789 hash value: 6814918571055915151
[rank0]:W1110 14:11:07.560000 21474 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36137 hash value: 14580905988001524408
[rank1]:W1110 14:11:07.858000 21475 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 36335 hash value: 10326290591803897105
[rank5]:W1110 14:11:08.087000 21481 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 33236 hash value: 9835614674694504284
[rank0]:W1110 14:11:08.279000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.282000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.284000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.286000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank5]:W1110 14:11:08.288000 21481 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 18105060839511562746
[rank2]:W1110 14:11:08.288000 21477 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5385889436139478811
[rank3]:W1110 14:11:08.288000 21479 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 8845860186250493752
[rank6]:W1110 14:11:08.288000 21482 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 12465652033505945645
[rank1]:W1110 14:11:08.288000 21475 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14729635040615891302
[rank7]:W1110 14:11:08.288000 21485 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1311711267518707223
[rank4]:W1110 14:11:08.288000 21480 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14915807116020889686
[rank0]:W1110 14:11:08.288000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.294000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.296000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.298000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 36335 hash value: 13215522533100337896
[rank0]:W1110 14:11:08.299000 21474 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5102777861255092634
[rank0]:W1110 14:11:08.304000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.305000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.306000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.307000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.308000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.309000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.310000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
[rank0]:W1110 14:11:08.311000 21474 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 10739636573918710171
2025-11-10:14:11:15 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:14:11:15 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
[rank0]:[W1110 14:11:16.673718092 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
