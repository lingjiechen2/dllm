The following values were not passed to `accelerate launch` and had defaults used instead:
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
2025-11-10:14:12:20 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:20 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:20 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:20 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:20 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:20 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:12:20 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:20 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:20.258963867 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:12:20.258969779 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:20 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:20 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:20 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:20 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:20.418152835 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:21 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:21 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:21 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:21 WARNING  [__main__:369]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-11-10:14:12:21 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:21 INFO     [__main__:450] Selected Tasks: ['humaneval_instruct']
2025-11-10:14:12:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:12:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
2025-11-10:14:12:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
[W1110 14:12:21.124517319 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:21.124932061 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:21.129315105 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:21 INFO     [evaluator:202] Setting random seed to 42 | Setting numpy seed to 42 | Setting torch manual seed to 42 | Setting fewshot manual seed to 42
2025-11-10:14:12:21 INFO     [evaluator:240] Initializing llada model, with arguments: {'pretrained': '/mnt/lustrenew/mllm_aligned/shared/models/huggingface/GSAI-ML/LLaDA-8B-Instruct',
        'is_check_greedy': False, 'mc_num': 1, 'max_new_tokens': 256, 'steps': 256, 'block_length': 256, 'cfg': 0.0,
        'confidence_eos_eot_inf': True}
[W1110 14:12:21.132457278 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[W1110 14:12:21.134711977 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:21 WARNING  [accelerate.utils.other:441] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.05it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.10it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.04it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.07it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.06it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:04,  1.04it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.12s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.13s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.11s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:02<00:04,  1.15s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.22s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:03<00:03,  1.25s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.24s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.27s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:04<00:02,  1.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.26s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.29s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.28s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.27s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:06<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]

Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.17s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.19s/it]
Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.21s/it]


Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.20s/it]
[rank0]:[W1110 14:12:33.080548900 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank6]:[W1110 14:12:33.080598830 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank4]:[W1110 14:12:33.080640372 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank7]:[W1110 14:12:33.080771384 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank1]:[W1110 14:12:33.080846262 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank2]:[W1110 14:12:33.080994264 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank3]:[W1110 14:12:33.081050787 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
[rank5]:[W1110 14:12:33.094296191 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())
2025-11-10:14:12:43 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:43 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:43 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:43 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 0...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 737.76it/s]
2025-11-10:14:12:43 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:43 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:43 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:43 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 6...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 710.13it/s]
2025-11-10:14:12:43 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:43 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:43 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:43 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 2...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 810.62it/s]
2025-11-10:14:12:44 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:44 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:44 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:44 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 3...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 840.14it/s]
2025-11-10:14:12:44 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:44 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:44 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:44 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 4...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 741.93it/s]
2025-11-10:14:12:45 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:45 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:45 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 7...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 834.27it/s]
2025-11-10:14:12:45 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:45 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:45 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 5...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 847.45it/s]
2025-11-10:14:12:45 INFO     [evaluator:305] humaneval_instruct: Using gen_kwargs: {'until': ['\nclass', '\ndef', '\n#', '\nif', '\nprint'], 'max_gen_toks': 1024, 'do_sample': False}
2025-11-10:14:12:45 INFO     [evaluator:320] num_fewshot has been set to 0 for humaneval_instruct in its config. Manual configuration will be ignored.
2025-11-10:14:12:45 WARNING  [evaluator:480] Chat template formatting change affects loglikelihood and multiple-choice tasks. See docs/chat-template-readme.md for details.
2025-11-10:14:12:45 INFO     [api.task:434] Building contexts for humaneval_instruct on rank 1...
  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:00<00:00, 694.59it/s]
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
2025-11-10:14:12:45 INFO     [evaluator:574] Running generate_until requests
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f892d7ca560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f892d7ca560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2215986560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f2215986560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f88f015e4d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f88f015e4d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fad746a24d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7fad746a24d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f588ff0e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f588ff0e560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map: 100%|██████████| 10/10 [00:00<00:00, 691.08 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f77a2072560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f43b0152560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f77a2072560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f43b0152560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f3fb9fde560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
Map: 100%|██████████| 10/10 [00:00<00:00, 745.34 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]2025-11-10:14:12:48 WARNING  [datasets.fingerprint:258] Parameter 'function'=<function LLaDAEvalHarness.generate_until.<locals>._tokenize at 0x7f3fb9fde560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.

Map: 100%|██████████| 10/10 [00:00<00:00, 844.77 examples/s]Map:   0%|          | 0/10 [00:00<?, ? examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 934.41 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 849.98 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 843.43 examples/s]
Map: 100%|██████████| 10/10 [00:00<00:00, 768.95 examples/s]Generating...:   0%|          | 0/10 [00:00<?, ?it/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Map: 100%|██████████| 10/10 [00:00<00:00, 722.92 examples/s]
Generating...:   0%|          | 0/10 [00:00<?, ?it/s]Generating...:  10%|█         | 1/10 [00:16<02:30, 16.67s/it]Generating...:  10%|█         | 1/10 [00:16<02:30, 16.68s/it]Generating...:  10%|█         | 1/10 [00:16<02:29, 16.67s/it]Generating...:  10%|█         | 1/10 [00:16<02:29, 16.66s/it]Generating...:  10%|█         | 1/10 [00:16<02:30, 16.67s/it]Generating...:  10%|█         | 1/10 [00:16<02:30, 16.68s/it]Generating...:  10%|█         | 1/10 [00:16<02:30, 16.68s/it]Generating...:  10%|█         | 1/10 [00:16<02:30, 16.67s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  20%|██        | 2/10 [00:33<02:14, 16.78s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 17.00s/it]Generating...:  30%|███       | 3/10 [00:50<01:58, 16.99s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  40%|████      | 4/10 [01:06<01:38, 16.45s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  50%|█████     | 5/10 [01:29<01:34, 18.82s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  60%|██████    | 6/10 [01:46<01:12, 18.20s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.42s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.42s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.42s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.42s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.43s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.43s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.42s/it]Generating...:  70%|███████   | 7/10 [02:02<00:52, 17.43s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  80%|████████  | 8/10 [02:18<00:34, 17.03s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...:  90%|█████████ | 9/10 [02:43<00:19, 19.54s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.95s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.95s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.96s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.95s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.95s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.95s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.96s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 20.96s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]
Generating...: 100%|██████████| 10/10 [03:07<00:00, 18.76s/it]






[rank5]:W1110 14:16:02.462000 30888 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37122 hash value: 7115621331849384013
[rank2]:W1110 14:16:03.683000 30885 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 34541 hash value: 14508591219556349949
[rank6]:W1110 14:16:03.833000 30889 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 39574 hash value: 12770746108768439318
[rank0]:W1110 14:16:03.983000 30883 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40288 hash value: 5155458457819887943
[rank7]:W1110 14:16:04.771000 30890 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 34117 hash value: 7398879064176414283
[rank3]:W1110 14:16:06.195000 30886 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 37339 hash value: 14705010330501286173
[rank1]:W1110 14:16:06.941000 30884 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 40914 hash value: 11027828934371686221
[rank4]:W1110 14:16:07.174000 30887 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 39052 hash value: 3809259693128094344
[rank0]:W1110 14:16:07.364000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.368000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.370000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.372000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.374000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank3]:W1110 14:16:07.374000 30886 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 1892600311987657058
[rank7]:W1110 14:16:07.374000 30890 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 5427842618415078366
[rank6]:W1110 14:16:07.374000 30889 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 15118374857218229164
[rank2]:W1110 14:16:07.374000 30885 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 4203407626583767199
[rank1]:W1110 14:16:07.374000 30884 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14728418002915937150
[rank5]:W1110 14:16:07.374000 30888 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14011187106182149858
[rank4]:W1110 14:16:07.374000 30887 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 14712373174771148005
[rank0]:W1110 14:16:07.377000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.383000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.385000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 40914 hash value: 6747967552569538067
[rank0]:W1110 14:16:07.386000 30883 site-packages/torch/distributed/distributed_c10d.py:2632] _object_to_tensor size: 291 hash value: 13926372034106151649
[rank0]:W1110 14:16:07.391000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.392000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.393000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.394000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.395000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.396000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.397000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
[rank0]:W1110 14:16:07.398000 30883 site-packages/torch/distributed/distributed_c10d.py:2644] _tensor_to_object size: 291 hash value: 797105825020897976
2025-11-10:14:16:14 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
2025-11-10:14:16:14 INFO     [loggers.evaluation_tracker:298] Saving per-sample results for: humaneval_instruct
[rank0]:[W1110 14:16:15.689871336 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
